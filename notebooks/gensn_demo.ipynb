{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a3e9010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensn.distributions import TrainableDistributionAdapter, Joint\n",
    "from gensn.variational import ELBOMarginal\n",
    "from gensn.parameters import TransformedParameter, PositiveDiagonal, Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d6a2d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.distributions import Normal, MultivariateNormal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da3a3f0-6496-44c2-877a-51e43049d564",
   "metadata": {},
   "source": [
    "## Example usage: learn Normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82fc8a0-3d0a-4787-8f7e-04bc9b7e4ad5",
   "metadata": {},
   "source": [
    "### Learn only the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "342a96b1-29ef-43ea-bc49-9eb2ad4c2792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the mean & std both learnable\n",
    "# std (scale) is set to be square of a parameter, thus ensuring positive value\n",
    "normal = TrainableDistributionAdapter(Normal, \n",
    "                               loc=nn.Parameter(torch.Tensor([0.0])), \n",
    "                               scale=torch.Tensor([2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f047e26-9865-439e-960c-35d10cf87447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('loc', tensor([0.])), ('scale', tensor([2.]))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebdde714-cb26-4019-9a1e-3ed68ad8528b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.], requires_grad=True)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(normal.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d75b75fc-f810-4ff1-b345-f41665612649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup target normal distribution to learn\n",
    "target = Normal(torch.Tensor([5.0]), torch.Tensor([2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbb2a510-507b-4bb9-8f9b-11a3d1b27b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam(normal.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "429c9425-d8bc-488d-b002-ac106a04aac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg logP: 2.1155459880828857, mean=tensor([4.9734])\n",
      "Neg logP: 2.1493823528289795, mean=tensor([4.9213])\n",
      "Neg logP: 2.211949348449707, mean=tensor([4.9919])\n",
      "Neg logP: 2.0131587982177734, mean=tensor([5.1923])\n",
      "Neg logP: 2.2272799015045166, mean=tensor([4.9601])\n",
      "Neg logP: 2.091372013092041, mean=tensor([5.1384])\n",
      "Neg logP: 2.0637428760528564, mean=tensor([5.2518])\n",
      "Neg logP: 2.1895248889923096, mean=tensor([5.1309])\n",
      "Neg logP: 2.1547136306762695, mean=tensor([4.9220])\n",
      "Neg logP: 2.1652190685272217, mean=tensor([5.2024])\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    targets = target.sample((100,))\n",
    "    nlogp = -normal(targets).mean()\n",
    "    mean = normal.loc.detach()\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f'Neg logP: {nlogp}, mean={mean}')\n",
    "    nlogp.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439fe712-7fa3-4a7a-b65c-b21e0332e2b9",
   "metadata": {},
   "source": [
    "### Learn both mean and stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7809263a-c323-4fe2-bcf1-d9d0ef714c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the mean & std both learnable\n",
    "# std (scale) is set to be square of a parameter, thus ensuring positive value\n",
    "normal = TrainableDistributionAdapter(Normal, \n",
    "                               loc=nn.Parameter(torch.Tensor([0.0])), \n",
    "                               scale=TransformedParameter(torch.Tensor([1.0]), lambda x: x**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "079c5d35-b3b6-41c6-b880-20f05a75f253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup target normal distribution to learn\n",
    "target = Normal(torch.Tensor([5]), torch.Tensor([8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe37983e-40d5-4932-8530-7965a4938788",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam(normal.parameters(), lr=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbd2836c-e6f0-416f-a851-3bdbd168b0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg logP: 4.5553154945373535, mean=tensor([6.3387]), std=tensor([37.1205])\n",
      "Neg logP: 3.6742889881134033, mean=tensor([5.9076]), std=tensor([13.9000])\n",
      "Neg logP: 3.489955186843872, mean=tensor([5.0773]), std=tensor([8.1060])\n",
      "Neg logP: 3.5183615684509277, mean=tensor([4.9740]), std=tensor([7.9830])\n",
      "Neg logP: 3.4774651527404785, mean=tensor([4.9874]), std=tensor([7.9501])\n",
      "Neg logP: 3.4990129470825195, mean=tensor([5.2858]), std=tensor([7.5867])\n",
      "Neg logP: 3.4986166954040527, mean=tensor([4.8456]), std=tensor([7.8560])\n",
      "Neg logP: 3.4958086013793945, mean=tensor([4.7768]), std=tensor([7.9615])\n",
      "Neg logP: 3.541909694671631, mean=tensor([4.8205]), std=tensor([7.5591])\n",
      "Neg logP: 3.453545570373535, mean=tensor([4.9117]), std=tensor([7.9764])\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    targets = target.sample((100,))\n",
    "    nlogp = -normal(targets).mean()\n",
    "    mean = normal.loc.detach()\n",
    "    std = normal.scale.value.detach()\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f'Neg logP: {nlogp}, mean={mean}, std={std}')\n",
    "    nlogp.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ebdfb9-b71b-4131-bff4-afe0cc963aba",
   "metadata": {},
   "source": [
    "### Learn both mean and stdev (same as above, but specified positionally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2fd10a4-5e15-479a-8cdf-d79e48014746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the mean & std both learnable\n",
    "# std (scale) is set to be square of a parameter, thus ensuring positive value\n",
    "normal = TrainableDistributionAdapter(Normal, \n",
    "                               nn.Parameter(torch.Tensor([0.0])), \n",
    "                               TransformedParameter(torch.Tensor([1.0]), lambda x: x**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3bcbc8b-ae5a-409e-8189-3fc127626849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup target normal distribution to learn\n",
    "target = Normal(torch.Tensor([5]), torch.Tensor([8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5a92ccf-bc7a-46d8-ab42-83118e6803be",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam(normal.parameters(), lr=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0725d790-8b17-4e59-b18b-bd500827adfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg logP: 4.551199436187744, mean=tensor([6.3952]), std=tensor([36.8402])\n",
      "Neg logP: 3.6840832233428955, mean=tensor([5.9138]), std=tensor([13.3099])\n",
      "Neg logP: 3.5708117485046387, mean=tensor([4.9872]), std=tensor([8.0710])\n",
      "Neg logP: 3.582395076751709, mean=tensor([5.1409]), std=tensor([8.0214])\n",
      "Neg logP: 3.56783390045166, mean=tensor([5.0813]), std=tensor([7.7231])\n",
      "Neg logP: 3.5683016777038574, mean=tensor([5.0931]), std=tensor([8.2432])\n",
      "Neg logP: 3.538731575012207, mean=tensor([4.8726]), std=tensor([8.0664])\n",
      "Neg logP: 3.4939463138580322, mean=tensor([5.1158]), std=tensor([7.8221])\n",
      "Neg logP: 3.5021615028381348, mean=tensor([5.0405]), std=tensor([7.8277])\n",
      "Neg logP: 3.493650436401367, mean=tensor([5.1432]), std=tensor([8.1597])\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    targets = target.sample((100,))\n",
    "    nlogp = -normal(targets).mean()\n",
    "    mean = normal._arg0.detach()\n",
    "    std = normal._arg1.value.detach()\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f'Neg logP: {nlogp}, mean={mean}, std={std}')\n",
    "    nlogp.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057437e7-add2-4d09-99ff-3711dd667f3f",
   "metadata": {},
   "source": [
    "# Conditional case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf10fc1f-e412-410e-abcb-435a4dfd4342",
   "metadata": {},
   "source": [
    "## Simple conditioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c02a881-565f-4acf-a07b-010e0031bc89",
   "metadata": {},
   "source": [
    "Now let us learn more complex relationship $p(z|x)$. Specifically, let $p(z|x) = \\mathcal{N}(f(x), \\sigma^2)$.\n",
    "\n",
    "For simplicity, we'll assume a simple linear mapping for $f(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a03c77-6e88-447a-b848-694c05115cfd",
   "metadata": {},
   "source": [
    "Prepare data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d527ba6-0b3f-45da-8d53-d66961bcd9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size):\n",
    "    x = torch.rand((batch_size, 1))\n",
    "    mu = -5 * x + 9\n",
    "    y = Normal(mu, scale=7).sample((batch_size,))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60912ec7-5621-4009-a984-a6da7c464633",
   "metadata": {},
   "source": [
    "Set up the conditional network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e054cdc5-a040-46eb-893a-c195ed186b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the mean & std both learnable\n",
    "# std (scale) is set to be abs of a parameter, thus ensuring positive value\n",
    "normal = TrainableDistributionAdapter(Normal, \n",
    "                               loc=nn.Linear(1, 1), \n",
    "                               scale=TransformedParameter(torch.Tensor([1.0]), torch.abs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e73da766-cdc7-4489-8f12-94f5889a71dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam(normal.parameters(), lr=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa6c9324-239c-4b22-896c-3438d041e7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg logP: 3.4037044048309326, params={'loc.weight': tensor([[0.5491]]), 'loc.bias': tensor([5.8328]), 'scale.parameter': tensor([7.8711])}\n",
      "Neg logP: 3.381765604019165, params={'loc.weight': tensor([[-1.3447]]), 'loc.bias': tensor([6.9216]), 'scale.parameter': tensor([7.5908])}\n",
      "Neg logP: 3.3738582134246826, params={'loc.weight': tensor([[-3.0239]]), 'loc.bias': tensor([7.8740]), 'scale.parameter': tensor([7.3113])}\n",
      "Neg logP: 3.354703187942505, params={'loc.weight': tensor([[-4.1132]]), 'loc.bias': tensor([8.4775]), 'scale.parameter': tensor([7.1333])}\n",
      "Neg logP: 3.3553593158721924, params={'loc.weight': tensor([[-4.6604]]), 'loc.bias': tensor([8.8195]), 'scale.parameter': tensor([7.0414])}\n",
      "Neg logP: 3.3557655811309814, params={'loc.weight': tensor([[-4.8893]]), 'loc.bias': tensor([8.9620]), 'scale.parameter': tensor([7.0145])}\n",
      "Neg logP: 3.3629565238952637, params={'loc.weight': tensor([[-4.9671]]), 'loc.bias': tensor([8.9659]), 'scale.parameter': tensor([7.0023])}\n",
      "Neg logP: 3.3732059001922607, params={'loc.weight': tensor([[-5.0098]]), 'loc.bias': tensor([8.9915]), 'scale.parameter': tensor([6.9950])}\n",
      "Neg logP: 3.3643898963928223, params={'loc.weight': tensor([[-4.9857]]), 'loc.bias': tensor([8.9825]), 'scale.parameter': tensor([7.0062])}\n",
      "Neg logP: 3.3619205951690674, params={'loc.weight': tensor([[-4.9796]]), 'loc.bias': tensor([9.0077]), 'scale.parameter': tensor([6.9961])}\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    x, y = get_batch(100)\n",
    "    nlogp = -normal(y, cond=x).mean()\n",
    "    params = {k:v.detach() for k,v in normal.state_dict().items()}\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f'Neg logP: {nlogp}, params={params}')\n",
    "    nlogp.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b214cea-c48a-48eb-b416-a70c8450c351",
   "metadata": {},
   "source": [
    "## One network with multiple outputs (returning dict):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4583b2d-4a12-475d-9c1c-0dc5dd6be5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalParams(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.core = nn.Linear(1, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        vals = self.core(x)\n",
    "        #loc, scale = vals.split((1,1), dim=1)\n",
    "        return dict(loc=vals[:,0:1], scale=(vals[:, 1:])**2)\n",
    "        #return dict(loc=loc, scale=scale**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02028ecf-19e7-47bb-8ac5-70bec962af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size):\n",
    "    x = torch.rand((batch_size, 1))\n",
    "    mu = -5 * x + 9\n",
    "    scale = (3 * x + 1)\n",
    "    model = Normal(mu, scale=scale)\n",
    "    y = model.sample((batch_size,))\n",
    "    return x, y, model.log_prob(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "445ebdae-679a-4ae7-ba4a-250732bd645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the mean & std both learnable\n",
    "# std (scale) is set to be square of a parameter, thus ensuring positive value\n",
    "normal = TrainableDistributionAdapter(Normal, _parameters=NormalParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68a3c3e8-d032-4c6a-98fc-b5aaf6e1ca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam(normal.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab641be3-668e-434e-8c7b-c39bbee50871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg logP: 879.588 (gt=2.240), params={'parameter_generator.core.weight': tensor([[-0.3289],\n",
      "        [-0.2346]]), 'parameter_generator.core.bias': tensor([ 0.6627, -0.3280])}\n",
      "Neg logP: 5.413 (gt=2.316), params={'parameter_generator.core.weight': tensor([[ 5.6894],\n",
      "        [-6.2071]]), 'parameter_generator.core.bias': tensor([ 6.6866, -6.3188])}\n",
      "Neg logP: 5.295 (gt=2.197), params={'parameter_generator.core.weight': tensor([[ 5.6862],\n",
      "        [-6.1472]]), 'parameter_generator.core.bias': tensor([ 6.6860, -6.2889])}\n",
      "Neg logP: 5.308 (gt=2.240), params={'parameter_generator.core.weight': tensor([[ 5.6816],\n",
      "        [-6.0674]]), 'parameter_generator.core.bias': tensor([ 6.6849, -6.2484])}\n",
      "Neg logP: 5.273 (gt=2.231), params={'parameter_generator.core.weight': tensor([[ 5.6757],\n",
      "        [-5.9698]]), 'parameter_generator.core.bias': tensor([ 6.6836, -6.1987])}\n",
      "Neg logP: 5.364 (gt=2.331), params={'parameter_generator.core.weight': tensor([[ 5.6686],\n",
      "        [-5.8548]]), 'parameter_generator.core.bias': tensor([ 6.6819, -6.1402])}\n",
      "Neg logP: 5.214 (gt=2.228), params={'parameter_generator.core.weight': tensor([[ 5.6600],\n",
      "        [-5.7218]]), 'parameter_generator.core.bias': tensor([ 6.6797, -6.0730])}\n",
      "Neg logP: 5.264 (gt=2.298), params={'parameter_generator.core.weight': tensor([[ 5.6495],\n",
      "        [-5.5699]]), 'parameter_generator.core.bias': tensor([ 6.6771, -5.9969])}\n",
      "Neg logP: 5.177 (gt=2.242), params={'parameter_generator.core.weight': tensor([[ 5.6371],\n",
      "        [-5.3993]]), 'parameter_generator.core.bias': tensor([ 6.6741, -5.9114])}\n",
      "Neg logP: 5.105 (gt=2.227), params={'parameter_generator.core.weight': tensor([[ 5.6226],\n",
      "        [-5.2104]]), 'parameter_generator.core.bias': tensor([ 6.6706, -5.8154])}\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    x, y, true_logp = get_batch(100)\n",
    "    nlogp = -normal(y, cond=x).mean()\n",
    "    params = {k:v.detach() for k,v in normal.state_dict().items()}\n",
    "    if i % 100 == 0:\n",
    "        print(f'Neg logP: {nlogp:0.3f} (gt={-true_logp.mean():0.3f}), params={params}')\n",
    "    nlogp.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700ce5da-dd6b-41b4-8369-bb1414bf2b81",
   "metadata": {},
   "source": [
    "## One network with multiple outputs (returning positionally):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "012d4071-af9a-4486-90fc-8d27a6c4f38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalParams(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.core = nn.Linear(1, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        vals = self.core(x)\n",
    "        #loc, scale = vals.split((1,1), dim=1)\n",
    "        return vals[:,0:1], (vals[:, 1:])**2\n",
    "        #return dict(loc=loc, scale=scale**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8001caf-c7e7-4f01-a53d-d5d73046e32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size):\n",
    "    x = torch.rand((batch_size, 1))\n",
    "    mu = -5 * x + 9\n",
    "    scale = (3 * x + 1)\n",
    "    model = Normal(mu, scale=scale)\n",
    "    y = model.sample((batch_size,))\n",
    "    return x, y, model.log_prob(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "233f1a5f-086e-46c4-b512-bdf177a3f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the mean & std both learnable\n",
    "# std (scale) is set to be square of a parameter, thus ensuring positive value\n",
    "normal = TrainableDistributionAdapter(Normal, _parameters=NormalParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0f74d010-d81d-42fc-a405-fe327cfdbd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam(normal.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d378f8c8-bde5-409c-a1bf-765aa0fb98a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg logP: 3234.762 (gt=2.269), params={'parameter_generator.core.weight': tensor([[-0.6911],\n",
      "        [-0.8402]]), 'parameter_generator.core.bias': tensor([0.3904, 0.9635])}\n",
      "Neg logP: 5.410 (gt=2.277), params={'parameter_generator.core.weight': tensor([[5.3150],\n",
      "        [5.1641]]), 'parameter_generator.core.bias': tensor([6.3980, 6.9668])}\n",
      "Neg logP: 5.463 (gt=2.332), params={'parameter_generator.core.weight': tensor([[5.3151],\n",
      "        [5.1626]]), 'parameter_generator.core.bias': tensor([6.3982, 6.9634])}\n",
      "Neg logP: 5.388 (gt=2.249), params={'parameter_generator.core.weight': tensor([[5.3148],\n",
      "        [5.1603]]), 'parameter_generator.core.bias': tensor([6.3979, 6.9586])}\n",
      "Neg logP: 5.361 (gt=2.222), params={'parameter_generator.core.weight': tensor([[5.3144],\n",
      "        [5.1575]]), 'parameter_generator.core.bias': tensor([6.3976, 6.9528])}\n",
      "Neg logP: 5.417 (gt=2.289), params={'parameter_generator.core.weight': tensor([[5.3139],\n",
      "        [5.1542]]), 'parameter_generator.core.bias': tensor([6.3973, 6.9459])}\n",
      "Neg logP: 5.416 (gt=2.278), params={'parameter_generator.core.weight': tensor([[5.3134],\n",
      "        [5.1505]]), 'parameter_generator.core.bias': tensor([6.3969, 6.9382])}\n",
      "Neg logP: 5.360 (gt=2.222), params={'parameter_generator.core.weight': tensor([[5.3128],\n",
      "        [5.1463]]), 'parameter_generator.core.bias': tensor([6.3965, 6.9295])}\n",
      "Neg logP: 5.399 (gt=2.273), params={'parameter_generator.core.weight': tensor([[5.3121],\n",
      "        [5.1417]]), 'parameter_generator.core.bias': tensor([6.3961, 6.9199])}\n",
      "Neg logP: 5.388 (gt=2.273), params={'parameter_generator.core.weight': tensor([[5.3114],\n",
      "        [5.1367]]), 'parameter_generator.core.bias': tensor([6.3956, 6.9093])}\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    x, y, true_logp = get_batch(100)\n",
    "    nlogp = -normal(y, cond=x).mean()\n",
    "    params = {k:v.detach() for k,v in normal.state_dict().items()}\n",
    "    if i % 100 == 0:\n",
    "        print(f'Neg logP: {nlogp:0.3f} (gt={-true_logp.mean():0.3f}), params={params}')\n",
    "    nlogp.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf2da13-725c-4f64-af19-1432284474c0",
   "metadata": {},
   "source": [
    "# Test sampling and joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11f3cc5a-da5f-4b28-8f33-ab7f44b38f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = TrainableDistributionAdapter(Normal, loc=nn.Parameter(torch.Tensor([5])),\n",
    "                                     scale=torch.Tensor([2]))\n",
    "\n",
    "linear = nn.Linear(1, 1)\n",
    "linear.weight.data = torch.Tensor([[-2]])\n",
    "linear.bias.data = torch.Tensor([6])\n",
    "conditional = TrainableDistributionAdapter(Normal, loc=linear, scale=torch.Tensor([1]))\n",
    "\n",
    "# create a joint distribution out of prior and conditional\n",
    "joint = Joint(prior, conditional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b33e734-1f33-4920-b997-864033282b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = joint.sample((10000,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5278584-1f7f-4741-a535-fb4115d17e24",
   "metadata": {},
   "source": [
    "Should be 5, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7c4ae965-786b-4cd3-8e29-dfbe404c87b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.0032), tensor(2.0049))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9ae6a0-b85f-4e96-87f1-8202de075983",
   "metadata": {},
   "source": [
    "Should be -4, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3440de7f-4bb6-4acb-9155-ca9abaa46e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-4.0130), tensor(4.1541))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.mean(), y.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fcba96-c6d3-4d27-aaf0-322efa2c69c1",
   "metadata": {},
   "source": [
    "# Simulating a multi-dimensional joint distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a202a9f-482b-44a6-bc01-64674e729ec7",
   "metadata": {},
   "source": [
    "### Define the ground-truth generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "654699e9-7213-4aa2-81a0-cb264b8e1ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_latents = 5\n",
    "gt_prior = TrainableDistributionAdapter(MultivariateNormal, \n",
    "                                        torch.ones([n_latents]),\n",
    "                                        torch.eye(n_latents))\n",
    "\n",
    "n_obs = 10\n",
    "features = torch.randn([n_latents, n_obs])\n",
    "\n",
    "feature_map = nn.Linear(n_latents, n_obs)\n",
    "feature_map.weight.data = features.T\n",
    "feature_map.bias.data.zero_()\n",
    "\n",
    "gt_conditional = TrainableDistributionAdapter(MultivariateNormal,\n",
    "                                              feature_map,\n",
    "                                              torch.eye(n_obs))\n",
    "\n",
    "gt_joint = Joint(gt_prior, gt_conditional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670797ef-bada-4481-8084-660ec61fbc15",
   "metadata": {},
   "source": [
    "### Now prepare a trainable model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fd1342-4ac6-419a-aef6-b9c9e92a2d16",
   "metadata": {},
   "source": [
    "class Covariance(nn.Module):\n",
    "    def __init__(self, n_dim):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "04f84ea4-6f14-499a-a42a-a789a4ddcd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_latents = 5\n",
    "model_prior = TrainableDistributionAdapter(MultivariateNormal, \n",
    "                                        nn.Parameter(torch.zeros([n_latents])),\n",
    "                                        PositiveDiagonal(n_latents))\n",
    "\n",
    "n_obs = 10\n",
    "features = torch.randn([n_latents, n_obs])\n",
    "\n",
    "feature_map = nn.Linear(n_latents, n_obs)\n",
    "#feature_map.weight.data = features.T\n",
    "#feature_map.bias.data.zero_()\n",
    "\n",
    "model_conditional = TrainableDistributionAdapter(MultivariateNormal,\n",
    "                                              feature_map,\n",
    "                                              torch.eye(n_obs))\n",
    "\n",
    "model_joint = Joint(model_prior, model_conditional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5aae5e-7c0f-4c05-be8e-a79c7b919d61",
   "metadata": {},
   "source": [
    "### Go ahead and train the joint model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6e0aa86e-e17f-4e8d-ad65-9721841e0186",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam(model_joint.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "92a32c09-c099-4593-b365-06f0a7f6000c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logp: -115.531 / GT logp: -21.522\n",
      "Model logp: -21.543 / GT logp: -21.270\n",
      "Model logp: -21.440 / GT logp: -21.152\n",
      "Model logp: -21.756 / GT logp: -21.236\n",
      "Model logp: -21.606 / GT logp: -21.260\n",
      "Model logp: -21.964 / GT logp: -21.495\n",
      "Model logp: -21.295 / GT logp: -21.023\n",
      "Model logp: -21.824 / GT logp: -21.071\n",
      "Model logp: -21.861 / GT logp: -21.264\n",
      "Model logp: -21.820 / GT logp: -21.440\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    samples = gt_joint.sample((100,))\n",
    "    gt_logl = gt_joint.log_prob(*samples).mean()\n",
    "    model_logl = model_joint.log_prob(*samples).mean()\n",
    "    if i % 100 == 0:\n",
    "        print(f'Model logp: {model_logl:.3f} / GT logp: {gt_logl:.3f}')\n",
    "    (-model_logl).backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23276263-918b-43d7-aca9-f18586b71b66",
   "metadata": {},
   "source": [
    "Checking the learned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2d55a492-57e4-4586-840f-f6619554a769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9212, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 1.2304, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 1.5738, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 1.0520, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 1.0407]], grad_fn=<DiagBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_joint.prior._arg1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b647e339-4091-4a06-bbf8-07e326e6dd76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('_arg0', tensor([0.8332, 0.8988, 0.9068, 0.8510, 0.7687])),\n",
       "             ('_arg1.D',\n",
       "              tensor([-0.9598, -1.1092, -1.2545,  1.0257, -1.0201]))])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_joint.prior.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d3b85c30-7306-4863-88d3-db6f66b8d720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('_arg1',\n",
       "              tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "                      [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "                      [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "                      [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "                      [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "                      [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "                      [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       "             ('_arg0.weight',\n",
       "              tensor([[-0.9050,  1.5812,  0.4306,  0.5078, -1.2745],\n",
       "                      [ 0.0943,  1.1739, -1.0866, -0.3643,  2.3895],\n",
       "                      [ 0.0312, -1.0371,  0.4130,  0.8667, -0.7674],\n",
       "                      [ 0.3733,  0.1724,  1.1796,  0.7658,  1.0307],\n",
       "                      [ 0.8435,  0.5376,  0.7024,  0.0312,  1.0789],\n",
       "                      [ 0.9290, -0.8582, -0.4515,  0.1531, -1.5645],\n",
       "                      [-1.3465, -1.0550,  0.1997, -0.6680, -0.0973],\n",
       "                      [ 0.8867, -0.4486,  1.0414, -2.4137, -1.3434],\n",
       "                      [-0.0167,  0.0350,  0.9185,  0.1877,  0.1135],\n",
       "                      [-1.4196, -1.0807, -0.8102, -1.8331, -0.5906]])),\n",
       "             ('_arg0.bias',\n",
       "              tensor([ 0.1173, -0.0364, -0.0189, -0.0170, -0.1497,  0.1020, -0.0704,  0.0326,\n",
       "                       0.0943,  0.1361]))])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_joint.conditional.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1e676c-2beb-48f4-ba7a-16e6ddc4dfe6",
   "metadata": {},
   "source": [
    "## Compute the posterior distriubtion via ELBO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1721c353-3fc8-48a6-9dd2-3076db090b9b",
   "metadata": {},
   "source": [
    "First, we'll train the posterior for the ground-truth model, using ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e8a1d168-d44e-4981-9377-44d8eadb6aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a posterior distribution\n",
    "features = torch.randn([n_obs, n_latents])\n",
    "\n",
    "linear_map = nn.Linear(n_obs, n_latents)\n",
    "\n",
    "posterior = TrainableDistributionAdapter(MultivariateNormal,\n",
    "                                              linear_map,\n",
    "                                              Covariance(n_latents, rank=5, eps=1e-6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f692ccc5-cbbe-4c2c-bea3-ba29d6f0e9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo_x = ELBOMarginal(gt_joint, posterior)\n",
    "\n",
    "# only training the posterior\n",
    "optim = Adam(posterior.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9b3ead08-3a46-495b-af06-7b56d838b713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model elbo: -726.289\n",
      "Model elbo: -42.695\n",
      "Model elbo: -24.010\n",
      "Model elbo: -20.559\n",
      "Model elbo: -20.051\n",
      "Model elbo: -19.918\n",
      "Model elbo: -19.868\n",
      "Model elbo: -19.838\n",
      "Model elbo: -19.816\n",
      "Model elbo: -19.827\n"
     ]
    }
   ],
   "source": [
    "z_sample, x_sample = gt_joint.sample((1000,))\n",
    "\n",
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    elbo = elbo_x(x_sample).mean()\n",
    "    if i % 100 == 0:\n",
    "        print(f'Model elbo: {elbo:.3f}')\n",
    "    (-elbo).backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76d032b-2ed2-4e09-a803-69f0463a4e0b",
   "metadata": {},
   "source": [
    "### Evaluate the posterior\n",
    "Here we will evaluate how good the posterior is (roughly) by sampling $\\hat{z}$ from the posterior $p(z|x)$ and evaluate the expected $\\log p(\\hat{z}, x)$.\n",
    "If $\\hat{z}$ approximates the true distribution over $z$, then $\\log p(\\hat{z}, x)$ will closely apprximate the expected $\\log p(z, x)$, which is negative of entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8581b914-7f75-4381-b3ec-d9acb6d486d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_sample, x_sample = gt_joint.sample((1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "07fea8fa-e733-4c3c-8a68-efeb80c95dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-21.3407, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ground-truth negative entropy\n",
    "gt_joint.log_prob(z_sample, x_sample).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5f49637e-67c9-4107-a824-7f56e93ac040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-21.3697, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample from the trained posterior\n",
    "z_hat = posterior.sample(cond=x_sample)\n",
    "\n",
    "# \n",
    "# negative entropy of the approximation\n",
    "gt_joint.log_prob(z_hat, x_sample).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b38dcd-8b0d-421f-a5ba-d70ff18656b0",
   "metadata": {},
   "source": [
    "## Compute the posterior distriubtion via direct fit to samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1684cb3d-c162-4308-8c09-81cb7caf746f",
   "metadata": {},
   "source": [
    "Now, we'll train the posterior for the ground-truth model by training directly on the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "21e531b0-2329-4c10-b025-d9821af8ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a posterior distribution\n",
    "features = torch.randn([n_obs, n_latents])\n",
    "\n",
    "linear_map = nn.Linear(n_obs, n_latents)\n",
    "\n",
    "posterior = TrainableDistributionAdapter(MultivariateNormal,\n",
    "                                              linear_map,\n",
    "                                              Covariance(n_latents, rank=5, eps=1e-6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d24a758e-6a95-4972-afa5-9501a301e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only training the posterior\n",
    "optim = Adam(posterior.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f093b35a-640d-4199-bbdc-b94d850a73d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogP: -142.656\n",
      "LogP: -7.445\n",
      "LogP: -6.255\n",
      "LogP: -4.870\n",
      "LogP: -4.178\n",
      "LogP: -3.914\n",
      "LogP: -3.563\n",
      "LogP: -3.272\n",
      "LogP: -3.157\n",
      "LogP: -2.958\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    z_sample, x_sample = gt_joint.sample((100,))\n",
    "    logp = posterior.log_prob(z_sample, cond=x_sample).mean()\n",
    "    if i % 100 == 0:\n",
    "        print(f'LogP: {logp:.3f}')\n",
    "    (-logp).backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105cf468-c7a1-4520-b0b8-6b88073a1f67",
   "metadata": {},
   "source": [
    "### Evaluate the posterior\n",
    "Here we will evaluate how good the posterior is (roughly) by sampling $\\hat{z}$ from the posterior $p(z|x)$ and evaluate the expected $\\log p(\\hat{z}, x)$.\n",
    "If $\\hat{z}$ approximates the true distribution over $z$, then $\\log p(\\hat{z}, x)$ will closely apprximate the expected $\\log p(z, x)$, which is negative of entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4ab93336-46a3-4eff-8ae3-8e2e5bf8f517",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_sample, x_sample = gt_joint.sample((1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "017c8ad0-0633-4ee9-aba0-431bbd096073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-21.2337, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ground-truth negative entropy\n",
    "gt_joint.log_prob(z_sample, x_sample).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8c51f065-1653-454c-b27f-b88b109b86eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-37.6097, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample from the trained posterior\n",
    "z_hat = posterior.sample(cond=x_sample)\n",
    "\n",
    "# \n",
    "# negative entropy of the approximation\n",
    "gt_joint.log_prob(z_hat, x_sample).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
