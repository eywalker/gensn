{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3e9010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensn.distributions import TrainableDistributionAdapter, Joint\n",
    "from gensn.variational import ELBOMarginal\n",
    "from gensn.parameters import TransformedParameter, PositiveDiagonal, Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6a2d2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.distributions import Normal, MultivariateNormal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da3a3f0-6496-44c2-877a-51e43049d564",
   "metadata": {},
   "source": [
    "## Example usage: learn Normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82fc8a0-3d0a-4787-8f7e-04bc9b7e4ad5",
   "metadata": {},
   "source": [
    "### Learn only the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342a96b1-29ef-43ea-bc49-9eb2ad4c2792",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make the mean & std both learnable\n",
    "# std (scale) is set to be square of a parameter, thus ensuring positive value\n",
    "normal = TrainableDistributionAdapter(Normal, \n",
    "                               loc=nn.Parameter(torch.Tensor([0.0])), \n",
    "                               scale=torch.Tensor([2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f047e26-9865-439e-960c-35d10cf87447",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "normal.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdde714-cb26-4019-9a1e-3ed68ad8528b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(normal.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75b75fc-f810-4ff1-b345-f41665612649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup target normal distribution to learn\n",
    "target = Normal(torch.Tensor([5.0]), torch.Tensor([2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb2a510-507b-4bb9-8f9b-11a3d1b27b06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optim = Adam(normal.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429c9425-d8bc-488d-b002-ac106a04aac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    targets = target.sample((100,))\n",
    "    nlogp = -normal(targets).mean()\n",
    "    mean = normal.loc.detach()\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f'Neg logP: {nlogp}, mean={mean}')\n",
    "    nlogp.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439fe712-7fa3-4a7a-b65c-b21e0332e2b9",
   "metadata": {},
   "source": [
    "### Learn both mean and stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7809263a-c323-4fe2-bcf1-d9d0ef714c4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make the mean & std both learnable\n",
    "# std (scale) is set to be square of a parameter, thus ensuring positive value\n",
    "normal = TrainableDistributionAdapter(Normal, \n",
    "                               loc=nn.Parameter(torch.Tensor([0.0])), \n",
    "                               scale=TransformedParameter(torch.Tensor([1.0]), lambda x: x**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079c5d35-b3b6-41c6-b880-20f05a75f253",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup target normal distribution to learn\n",
    "target = Normal(torch.Tensor([5]), torch.Tensor([8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe37983e-40d5-4932-8530-7965a4938788",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optim = Adam(normal.parameters(), lr=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd2836c-e6f0-416f-a851-3bdbd168b0fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    targets = target.sample((100,))\n",
    "    nlogp = -normal(targets).mean()\n",
    "    mean = normal.loc.detach()\n",
    "    std = normal.scale.value.detach()\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f'Neg logP: {nlogp}, mean={mean}, std={std}')\n",
    "    nlogp.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ebdfb9-b71b-4131-bff4-afe0cc963aba",
   "metadata": {},
   "source": [
    "### Learn both mean and stdev (same as above, but specified positionally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fd10a4-5e15-479a-8cdf-d79e48014746",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make the mean & std both learnable\n",
    "# std (scale) is set to be square of a parameter, thus ensuring positive value\n",
    "normal = TrainableDistributionAdapter(Normal, \n",
    "                               nn.Parameter(torch.Tensor([0.0])), \n",
    "                               TransformedParameter(torch.Tensor([1.0]), lambda x: x**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bcbc8b-ae5a-409e-8189-3fc127626849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup target normal distribution to learn\n",
    "target = Normal(torch.Tensor([5]), torch.Tensor([8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a92ccf-bc7a-46d8-ab42-83118e6803be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optim = Adam(normal.parameters(), lr=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0725d790-8b17-4e59-b18b-bd500827adfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    targets = target.sample((100,))\n",
    "    nlogp = -normal(targets).mean()\n",
    "    mean = normal._arg0.detach()\n",
    "    std = normal._arg1.value.detach()\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f'Neg logP: {nlogp}, mean={mean}, std={std}')\n",
    "    nlogp.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057437e7-add2-4d09-99ff-3711dd667f3f",
   "metadata": {},
   "source": [
    "# Conditional case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf10fc1f-e412-410e-abcb-435a4dfd4342",
   "metadata": {},
   "source": [
    "## Simple conditioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c02a881-565f-4acf-a07b-010e0031bc89",
   "metadata": {},
   "source": [
    "Now let us learn more complex relationship $p(z|x)$. Specifically, let $p(z|x) = \\mathcal{N}(f(x), \\sigma^2)$.\n",
    "\n",
    "For simplicity, we'll assume a simple linear mapping for $f(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a03c77-6e88-447a-b848-694c05115cfd",
   "metadata": {},
   "source": [
    "Prepare data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d527ba6-0b3f-45da-8d53-d66961bcd9a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_batch(batch_size):\n",
    "    x = torch.rand((batch_size, 1))\n",
    "    mu = -5 * x + 9\n",
    "    y = Normal(mu, scale=7).sample((batch_size,))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60912ec7-5621-4009-a984-a6da7c464633",
   "metadata": {},
   "source": [
    "Set up the conditional network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e054cdc5-a040-46eb-893a-c195ed186b3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make the mean & std both learnable\n",
    "# std (scale) is set to be abs of a parameter, thus ensuring positive value\n",
    "normal = TrainableDistributionAdapter(Normal, \n",
    "                               loc=nn.Linear(1, 1), \n",
    "                               scale=TransformedParameter(torch.Tensor([1.0]), torch.abs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73da766-cdc7-4489-8f12-94f5889a71dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optim = Adam(normal.parameters(), lr=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6c9324-239c-4b22-896c-3438d041e7d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    x, y = get_batch(100)\n",
    "    nlogp = -normal(y, cond=x).mean()\n",
    "    params = {k:v.detach() for k,v in normal.state_dict().items()}\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f'Neg logP: {nlogp}, params={params}')\n",
    "    nlogp.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b214cea-c48a-48eb-b416-a70c8450c351",
   "metadata": {},
   "source": [
    "## One network with multiple outputs (returning dict):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4583b2d-4a12-475d-9c1c-0dc5dd6be5f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NormalParams(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.core = nn.Linear(1, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        vals = self.core(x)\n",
    "        #loc, scale = vals.split((1,1), dim=1)\n",
    "        return dict(loc=vals[:,0:1], scale=(vals[:, 1:])**2)\n",
    "        #return dict(loc=loc, scale=scale**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02028ecf-19e7-47bb-8ac5-70bec962af6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_batch(batch_size):\n",
    "    x = torch.rand((batch_size, 1))\n",
    "    mu = -5 * x + 9\n",
    "    scale = (3 * x + 1)\n",
    "    model = Normal(mu, scale=scale)\n",
    "    y = model.sample((batch_size,))\n",
    "    return x, y, model.log_prob(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445ebdae-679a-4ae7-ba4a-250732bd645b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make the mean & std both learnable\n",
    "# std (scale) is set to be square of a parameter, thus ensuring positive value\n",
    "normal = TrainableDistributionAdapter(Normal, _parameters=NormalParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a3c3e8-d032-4c6a-98fc-b5aaf6e1ca6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optim = Adam(normal.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab641be3-668e-434e-8c7b-c39bbee50871",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    x, y, true_logp = get_batch(100)\n",
    "    nlogp = -normal(y, cond=x).mean()\n",
    "    params = {k:v.detach() for k,v in normal.state_dict().items()}\n",
    "    if i % 100 == 0:\n",
    "        print(f'Neg logP: {nlogp:0.3f} (gt={-true_logp.mean():0.3f}), params={params}')\n",
    "    nlogp.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700ce5da-dd6b-41b4-8369-bb1414bf2b81",
   "metadata": {},
   "source": [
    "## One network with multiple outputs (returning positionally):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012d4071-af9a-4486-90fc-8d27a6c4f38e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NormalParams(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.core = nn.Linear(1, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        vals = self.core(x)\n",
    "        #loc, scale = vals.split((1,1), dim=1)\n",
    "        return vals[:,0:1], (vals[:, 1:])**2\n",
    "        #return dict(loc=loc, scale=scale**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8001caf-c7e7-4f01-a53d-d5d73046e32d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_batch(batch_size):\n",
    "    x = torch.rand((batch_size, 1))\n",
    "    mu = -5 * x + 9\n",
    "    scale = (3 * x + 1)\n",
    "    model = Normal(mu, scale=scale)\n",
    "    y = model.sample((batch_size,))\n",
    "    return x, y, model.log_prob(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233f1a5f-086e-46c4-b512-bdf177a3f4f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make the mean & std both learnable\n",
    "# std (scale) is set to be square of a parameter, thus ensuring positive value\n",
    "normal = TrainableDistributionAdapter(Normal, _parameters=NormalParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f74d010-d81d-42fc-a405-fe327cfdbd1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optim = Adam(normal.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d378f8c8-bde5-409c-a1bf-765aa0fb98a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    x, y, true_logp = get_batch(100)\n",
    "    nlogp = -normal(y, cond=x).mean()\n",
    "    params = {k:v.detach() for k,v in normal.state_dict().items()}\n",
    "    if i % 100 == 0:\n",
    "        print(f'Neg logP: {nlogp:0.3f} (gt={-true_logp.mean():0.3f}), params={params}')\n",
    "    nlogp.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf2da13-725c-4f64-af19-1432284474c0",
   "metadata": {},
   "source": [
    "# Test sampling and joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f3cc5a-da5f-4b28-8f33-ab7f44b38f62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prior = TrainableDistributionAdapter(Normal, loc=nn.Parameter(torch.Tensor([5])),\n",
    "                                     scale=torch.Tensor([2]))\n",
    "\n",
    "linear = nn.Linear(1, 1)\n",
    "linear.weight.data = torch.Tensor([[-2]])\n",
    "linear.bias.data = torch.Tensor([6])\n",
    "conditional = TrainableDistributionAdapter(Normal, loc=linear, scale=torch.Tensor([1]))\n",
    "\n",
    "# create a joint distribution out of prior and conditional\n",
    "joint = Joint(prior, conditional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b33e734-1f33-4920-b997-864033282b11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x, y = joint.sample((10000,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5278584-1f7f-4741-a535-fb4115d17e24",
   "metadata": {},
   "source": [
    "Should be 5, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4ae965-786b-4cd3-8e29-dfbe404c87b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9ae6a0-b85f-4e96-87f1-8202de075983",
   "metadata": {},
   "source": [
    "Should be -4, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3440de7f-4bb6-4acb-9155-ca9abaa46e68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y.mean(), y.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fcba96-c6d3-4d27-aaf0-322efa2c69c1",
   "metadata": {},
   "source": [
    "# Simulating a multi-dimensional joint distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a202a9f-482b-44a6-bc01-64674e729ec7",
   "metadata": {},
   "source": [
    "### Define the ground-truth generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654699e9-7213-4aa2-81a0-cb264b8e1ad9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_latents = 5\n",
    "gt_prior = TrainableDistributionAdapter(MultivariateNormal, \n",
    "                                        torch.ones([n_latents]),\n",
    "                                        torch.eye(n_latents))\n",
    "\n",
    "n_obs = 10\n",
    "features = torch.randn([n_latents, n_obs])\n",
    "\n",
    "feature_map = nn.Linear(n_latents, n_obs)\n",
    "feature_map.weight.data = features.T\n",
    "feature_map.bias.data.zero_()\n",
    "\n",
    "gt_conditional = TrainableDistributionAdapter(MultivariateNormal,\n",
    "                                              feature_map,\n",
    "                                              torch.eye(n_obs))\n",
    "\n",
    "gt_joint = Joint(gt_prior, gt_conditional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670797ef-bada-4481-8084-660ec61fbc15",
   "metadata": {},
   "source": [
    "### Now prepare a trainable model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fd1342-4ac6-419a-aef6-b9c9e92a2d16",
   "metadata": {},
   "source": [
    "class Covariance(nn.Module):\n",
    "    def __init__(self, n_dim):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f84ea4-6f14-499a-a42a-a789a4ddcd1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_latents = 5\n",
    "model_prior = TrainableDistributionAdapter(MultivariateNormal, \n",
    "                                        nn.Parameter(torch.zeros([n_latents])),\n",
    "                                        PositiveDiagonal(n_latents))\n",
    "\n",
    "n_obs = 10\n",
    "features = torch.randn([n_latents, n_obs])\n",
    "\n",
    "feature_map = nn.Linear(n_latents, n_obs)\n",
    "#feature_map.weight.data = features.T\n",
    "#feature_map.bias.data.zero_()\n",
    "\n",
    "model_conditional = TrainableDistributionAdapter(MultivariateNormal,\n",
    "                                              feature_map,\n",
    "                                              torch.eye(n_obs))\n",
    "\n",
    "model_joint = Joint(model_prior, model_conditional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5aae5e-7c0f-4c05-be8e-a79c7b919d61",
   "metadata": {},
   "source": [
    "### Go ahead and train the joint model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0aa86e-e17f-4e8d-ad65-9721841e0186",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optim = Adam(model_joint.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a32c09-c099-4593-b365-06f0a7f6000c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    samples = gt_joint.sample((100,))\n",
    "    gt_logl = gt_joint.log_prob(*samples).mean()\n",
    "    model_logl = model_joint.log_prob(*samples).mean()\n",
    "    if i % 100 == 0:\n",
    "        print(f'Model logp: {model_logl:.3f} / GT logp: {gt_logl:.3f}')\n",
    "    (-model_logl).backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23276263-918b-43d7-aca9-f18586b71b66",
   "metadata": {},
   "source": [
    "Checking the learned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d55a492-57e4-4586-840f-f6619554a769",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_joint.prior._arg1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b647e339-4091-4a06-bbf8-07e326e6dd76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_joint.prior.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b85c30-7306-4863-88d3-db6f66b8d720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_joint.conditional.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1e676c-2beb-48f4-ba7a-16e6ddc4dfe6",
   "metadata": {},
   "source": [
    "## Compute the posterior distriubtion via ELBO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1721c353-3fc8-48a6-9dd2-3076db090b9b",
   "metadata": {},
   "source": [
    "First, we'll train the posterior for the ground-truth model, using ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a1d168-d44e-4981-9377-44d8eadb6aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare a posterior distribution\n",
    "features = torch.randn([n_obs, n_latents])\n",
    "\n",
    "linear_map = nn.Linear(n_obs, n_latents)\n",
    "\n",
    "posterior = TrainableDistributionAdapter(MultivariateNormal,\n",
    "                                              linear_map,\n",
    "                                              Covariance(n_latents, rank=5, eps=1e-6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f692ccc5-cbbe-4c2c-bea3-ba29d6f0e9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo_x = ELBOMarginal(gt_joint, posterior)\n",
    "\n",
    "# only training the posterior\n",
    "optim = Adam(posterior.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3ead08-3a46-495b-af06-7b56d838b713",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_sample, x_sample = gt_joint.sample((1000,))\n",
    "\n",
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    elbo = elbo_x(x_sample).mean()\n",
    "    if i % 100 == 0:\n",
    "        print(f'Model elbo: {elbo:.3f}')\n",
    "    (-elbo).backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76d032b-2ed2-4e09-a803-69f0463a4e0b",
   "metadata": {},
   "source": [
    "### Evaluate the posterior\n",
    "Here we will evaluate how good the posterior is (roughly) by sampling $\\hat{z}$ from the posterior $p(z|x)$ and evaluate the expected $\\log p(\\hat{z}, x)$.\n",
    "If $\\hat{z}$ approximates the true distribution over $z$, then $\\log p(\\hat{z}, x)$ will closely apprximate the expected $\\log p(z, x)$, which is negative of entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8581b914-7f75-4381-b3ec-d9acb6d486d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_sample, x_sample = gt_joint.sample((1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fea8fa-e733-4c3c-8a68-efeb80c95dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground-truth negative entropy\n",
    "gt_joint.log_prob(z_sample, x_sample).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f49637e-67c9-4107-a824-7f56e93ac040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the trained posterior\n",
    "z_hat = posterior.sample(cond=x_sample)\n",
    "\n",
    "# \n",
    "# negative entropy of the approximation\n",
    "gt_joint.log_prob(z_hat, x_sample).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b38dcd-8b0d-421f-a5ba-d70ff18656b0",
   "metadata": {},
   "source": [
    "## Compute the posterior distriubtion via direct fit to samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1684cb3d-c162-4308-8c09-81cb7caf746f",
   "metadata": {},
   "source": [
    "Now, we'll train the posterior for the ground-truth model by training directly on the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e531b0-2329-4c10-b025-d9821af8ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a posterior distribution\n",
    "features = torch.randn([n_obs, n_latents])\n",
    "\n",
    "linear_map = nn.Linear(n_obs, n_latents)\n",
    "\n",
    "posterior = TrainableDistributionAdapter(MultivariateNormal,\n",
    "                                              linear_map,\n",
    "                                              Covariance(n_latents, rank=5, eps=1e-6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24a758e-6a95-4972-afa5-9501a301e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only training the posterior\n",
    "optim = Adam(posterior.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f093b35a-640d-4199-bbdc-b94d850a73d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    z_sample, x_sample = gt_joint.sample((100,))\n",
    "    logp = posterior.log_prob(z_sample, cond=x_sample).mean()\n",
    "    if i % 100 == 0:\n",
    "        print(f'LogP: {logp:.3f}')\n",
    "    (-logp).backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105cf468-c7a1-4520-b0b8-6b88073a1f67",
   "metadata": {},
   "source": [
    "### Evaluate the posterior\n",
    "Here we will evaluate how good the posterior is (roughly) by sampling $\\hat{z}$ from the posterior $p(z|x)$ and evaluate the expected $\\log p(\\hat{z}, x)$.\n",
    "If $\\hat{z}$ approximates the true distribution over $z$, then $\\log p(\\hat{z}, x)$ will closely apprximate the expected $\\log p(z, x)$, which is negative of entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab93336-46a3-4eff-8ae3-8e2e5bf8f517",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_sample, x_sample = gt_joint.sample((1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017c8ad0-0633-4ee9-aba0-431bbd096073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground-truth negative entropy\n",
    "gt_joint.log_prob(z_sample, x_sample).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c51f065-1653-454c-b27f-b88b109b86eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the trained posterior\n",
    "z_hat = posterior.sample(cond=x_sample)\n",
    "\n",
    "# \n",
    "# negative entropy of the approximation\n",
    "gt_joint.log_prob(z_hat, x_sample).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
