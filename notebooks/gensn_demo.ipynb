{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a3e9010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensn.distributions import TrainableDistributionAdapter, Joint\n",
    "from gensn.variational import ELBOMarginal\n",
    "from gensn.parameters import TransformedParameter, PositiveDiagonal, Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d6a2d2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.distributions import Normal, MultivariateNormal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da3a3f0-6496-44c2-877a-51e43049d564",
   "metadata": {},
   "source": [
    "## Example usage: learn Normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82fc8a0-3d0a-4787-8f7e-04bc9b7e4ad5",
   "metadata": {},
   "source": [
    "### Learn only the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "342a96b1-29ef-43ea-bc49-9eb2ad4c2792",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make the mean & std both learnable\n",
    "# std (scale) is set to be square of a parameter, thus ensuring positive value\n",
    "normal = TrainableDistributionAdapter(Normal, \n",
    "                               loc=nn.Parameter(torch.Tensor([0.0])), \n",
    "                               scale=torch.Tensor([2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f047e26-9865-439e-960c-35d10cf87447",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('loc', tensor([0.])), ('scale', tensor([2.]))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebdde714-cb26-4019-9a1e-3ed68ad8528b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.], requires_grad=True)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(normal.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d75b75fc-f810-4ff1-b345-f41665612649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup target normal distribution to learn\n",
    "target = Normal(torch.Tensor([5.0]), torch.Tensor([2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbb2a510-507b-4bb9-8f9b-11a3d1b27b06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optim = Adam(normal.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "429c9425-d8bc-488d-b002-ac106a04aac6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg logP: 2.0490012168884277, mean=tensor([4.9324])\n",
      "Neg logP: 2.1136226654052734, mean=tensor([4.9247])\n",
      "Neg logP: 2.0624942779541016, mean=tensor([5.0990])\n",
      "Neg logP: 2.133841037750244, mean=tensor([5.0030])\n",
      "Neg logP: 2.133676290512085, mean=tensor([4.9113])\n",
      "Neg logP: 2.116976737976074, mean=tensor([5.1844])\n",
      "Neg logP: 2.1371238231658936, mean=tensor([4.8856])\n",
      "Neg logP: 2.0871145725250244, mean=tensor([5.2740])\n",
      "Neg logP: 2.124019145965576, mean=tensor([5.1323])\n",
      "Neg logP: 2.1224982738494873, mean=tensor([4.9836])\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    targets = target.sample((100,))\n",
    "    nlogp = -normal(targets).mean()\n",
    "    mean = normal.loc.detach()\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f'Neg logP: {nlogp}, mean={mean}')\n",
    "    nlogp.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439fe712-7fa3-4a7a-b65c-b21e0332e2b9",
   "metadata": {},
   "source": [
    "### Learn both mean and stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7809263a-c323-4fe2-bcf1-d9d0ef714c4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make the mean & std both learnable\n",
    "# std (scale) is set to be square of a parameter, thus ensuring positive value\n",
    "normal = TrainableDistributionAdapter(Normal, \n",
    "                               loc=nn.Parameter(torch.Tensor([0.0])), \n",
    "                               scale=TransformedParameter(torch.Tensor([1.0]), lambda x: x**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "079c5d35-b3b6-41c6-b880-20f05a75f253",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup target normal distribution to learn\n",
    "target = Normal(torch.Tensor([5]), torch.Tensor([8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe37983e-40d5-4932-8530-7965a4938788",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optim = Adam(normal.parameters(), lr=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbd2836c-e6f0-416f-a851-3bdbd168b0fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg logP: 4.549954891204834, mean=tensor([6.2335]), std=tensor([36.8581])\n",
      "Neg logP: 3.7240371704101562, mean=tensor([5.9152]), std=tensor([14.1016])\n",
      "Neg logP: 3.4181244373321533, mean=tensor([5.0096]), std=tensor([7.9622])\n",
      "Neg logP: 3.5762038230895996, mean=tensor([5.0104]), std=tensor([7.8075])\n",
      "Neg logP: 3.583933115005493, mean=tensor([4.8745]), std=tensor([7.9401])\n",
      "Neg logP: 3.420821189880371, mean=tensor([5.1534]), std=tensor([8.1514])\n",
      "Neg logP: 3.516340970993042, mean=tensor([5.0027]), std=tensor([8.2877])\n",
      "Neg logP: 3.6439175605773926, mean=tensor([5.0950]), std=tensor([8.2710])\n",
      "Neg logP: 3.349782943725586, mean=tensor([5.1073]), std=tensor([8.0211])\n",
      "Neg logP: 3.498356819152832, mean=tensor([5.1375]), std=tensor([7.9661])\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    targets = target.sample((100,))\n",
    "    nlogp = -normal(targets).mean()\n",
    "    mean = normal.loc.detach()\n",
    "    std = normal.scale.value.detach()\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f'Neg logP: {nlogp}, mean={mean}, std={std}')\n",
    "    nlogp.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ebdfb9-b71b-4131-bff4-afe0cc963aba",
   "metadata": {},
   "source": [
    "### Learn both mean and stdev (same as above, but specified positionally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2fd10a4-5e15-479a-8cdf-d79e48014746",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make the mean & std both learnable\n",
    "# std (scale) is set to be square of a parameter, thus ensuring positive value\n",
    "normal = TrainableDistributionAdapter(Normal, \n",
    "                               nn.Parameter(torch.Tensor([0.0])), \n",
    "                               TransformedParameter(torch.Tensor([1.0]), lambda x: x**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3bcbc8b-ae5a-409e-8189-3fc127626849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup target normal distribution to learn\n",
    "target = Normal(torch.Tensor([5]), torch.Tensor([8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5a92ccf-bc7a-46d8-ab42-83118e6803be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optim = Adam(normal.parameters(), lr=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0725d790-8b17-4e59-b18b-bd500827adfe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg logP: 4.557194709777832, mean=tensor([6.3509]), std=tensor([37.3025])\n",
      "Neg logP: 3.725822687149048, mean=tensor([5.8476]), std=tensor([14.1823])\n",
      "Neg logP: 3.412493944168091, mean=tensor([5.0580]), std=tensor([8.1510])\n",
      "Neg logP: 3.450681447982788, mean=tensor([4.7119]), std=tensor([7.9666])\n",
      "Neg logP: 3.5525712966918945, mean=tensor([4.7050]), std=tensor([8.1479])\n",
      "Neg logP: 3.6055593490600586, mean=tensor([5.1155]), std=tensor([8.0523])\n",
      "Neg logP: 3.575485944747925, mean=tensor([4.8290]), std=tensor([7.9291])\n",
      "Neg logP: 3.5787813663482666, mean=tensor([4.8853]), std=tensor([7.9831])\n",
      "Neg logP: 3.444620370864868, mean=tensor([4.5961]), std=tensor([7.7306])\n",
      "Neg logP: 3.4562206268310547, mean=tensor([5.1633]), std=tensor([7.7799])\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    targets = target.sample((100,))\n",
    "    nlogp = -normal(targets).mean()\n",
    "    mean = normal._arg0.detach()\n",
    "    std = normal._arg1.value.detach()\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f'Neg logP: {nlogp}, mean={mean}, std={std}')\n",
    "    nlogp.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057437e7-add2-4d09-99ff-3711dd667f3f",
   "metadata": {},
   "source": [
    "# Conditional case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf10fc1f-e412-410e-abcb-435a4dfd4342",
   "metadata": {},
   "source": [
    "## Simple conditioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c02a881-565f-4acf-a07b-010e0031bc89",
   "metadata": {},
   "source": [
    "Now let us learn more complex relationship $p(z|x)$. Specifically, let $p(z|x) = \\mathcal{N}(f(x), \\sigma^2)$.\n",
    "\n",
    "For simplicity, we'll assume a simple linear mapping for $f(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a03c77-6e88-447a-b848-694c05115cfd",
   "metadata": {},
   "source": [
    "Prepare data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d527ba6-0b3f-45da-8d53-d66961bcd9a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_batch(batch_size):\n",
    "    x = torch.rand((batch_size, 1))\n",
    "    mu = -5 * x + 9\n",
    "    y = Normal(mu, scale=7).sample((batch_size,))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60912ec7-5621-4009-a984-a6da7c464633",
   "metadata": {},
   "source": [
    "Set up the conditional network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e054cdc5-a040-46eb-893a-c195ed186b3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make the mean & std both learnable\n",
    "# std (scale) is set to be abs of a parameter, thus ensuring positive value\n",
    "normal = TrainableDistributionAdapter(Normal, \n",
    "                               loc=nn.Linear(1, 1), \n",
    "                               scale=TransformedParameter(torch.Tensor([1.0]), torch.abs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e73da766-cdc7-4489-8f12-94f5889a71dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optim = Adam(normal.parameters(), lr=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa6c9324-239c-4b22-896c-3438d041e7d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg logP: 3.406355142593384, params={'loc.weight': tensor([[0.6536]]), 'loc.bias': tensor([5.7188]), 'scale.parameter': tensor([7.9252])}\n",
      "Neg logP: 3.37754225730896, params={'loc.weight': tensor([[-1.3806]]), 'loc.bias': tensor([6.9814]), 'scale.parameter': tensor([7.6032])}\n",
      "Neg logP: 3.378467559814453, params={'loc.weight': tensor([[-3.2067]]), 'loc.bias': tensor([7.9680]), 'scale.parameter': tensor([7.2977])}\n",
      "Neg logP: 3.3775687217712402, params={'loc.weight': tensor([[-4.3148]]), 'loc.bias': tensor([8.5797]), 'scale.parameter': tensor([7.1092])}\n",
      "Neg logP: 3.3665573596954346, params={'loc.weight': tensor([[-4.7426]]), 'loc.bias': tensor([8.8968]), 'scale.parameter': tensor([7.0250])}\n",
      "Neg logP: 3.3628203868865967, params={'loc.weight': tensor([[-4.9615]]), 'loc.bias': tensor([8.9708]), 'scale.parameter': tensor([7.0068])}\n",
      "Neg logP: 3.3707962036132812, params={'loc.weight': tensor([[-4.9755]]), 'loc.bias': tensor([9.0007]), 'scale.parameter': tensor([7.0055])}\n",
      "Neg logP: 3.3539960384368896, params={'loc.weight': tensor([[-4.9969]]), 'loc.bias': tensor([9.0230]), 'scale.parameter': tensor([6.9986])}\n",
      "Neg logP: 3.3673198223114014, params={'loc.weight': tensor([[-5.0112]]), 'loc.bias': tensor([9.0191]), 'scale.parameter': tensor([7.0043])}\n",
      "Neg logP: 3.369715690612793, params={'loc.weight': tensor([[-4.9753]]), 'loc.bias': tensor([8.9865]), 'scale.parameter': tensor([6.9923])}\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    x, y = get_batch(100)\n",
    "    nlogp = -normal(y, cond=x).mean()\n",
    "    params = {k:v.detach() for k,v in normal.state_dict().items()}\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f'Neg logP: {nlogp}, params={params}')\n",
    "    nlogp.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b214cea-c48a-48eb-b416-a70c8450c351",
   "metadata": {},
   "source": [
    "## One network with multiple outputs (returning dict):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4583b2d-4a12-475d-9c1c-0dc5dd6be5f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NormalParams(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.core = nn.Linear(1, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        vals = self.core(x)\n",
    "        #loc, scale = vals.split((1,1), dim=1)\n",
    "        return dict(loc=vals[:,0:1], scale=(vals[:, 1:])**2)\n",
    "        #return dict(loc=loc, scale=scale**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02028ecf-19e7-47bb-8ac5-70bec962af6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_batch(batch_size):\n",
    "    x = torch.rand((batch_size, 1))\n",
    "    mu = -5 * x + 9\n",
    "    scale = (3 * x + 1)\n",
    "    model = Normal(mu, scale=scale)\n",
    "    y = model.sample((batch_size,))\n",
    "    return x, y, model.log_prob(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "445ebdae-679a-4ae7-ba4a-250732bd645b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make the mean & std both learnable\n",
    "# std (scale) is set to be square of a parameter, thus ensuring positive value\n",
    "normal = TrainableDistributionAdapter(Normal, _parameters=NormalParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "68a3c3e8-d032-4c6a-98fc-b5aaf6e1ca6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optim = Adam(normal.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ab641be3-668e-434e-8c7b-c39bbee50871",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg logP: 652.502 (gt=2.303), params={'parameter_generator.core.weight': tensor([[-0.3438],\n",
      "        [-0.8978]]), 'parameter_generator.core.bias': tensor([ 0.9711, -0.2252])}\n",
      "Neg logP: 5.496 (gt=2.332), params={'parameter_generator.core.weight': tensor([[ 5.6848],\n",
      "        [-6.8267]]), 'parameter_generator.core.bias': tensor([ 7.0001, -6.2144])}\n",
      "Neg logP: 5.469 (gt=2.330), params={'parameter_generator.core.weight': tensor([[ 5.6762],\n",
      "        [-6.6893]]), 'parameter_generator.core.bias': tensor([ 6.9992, -6.1804])}\n",
      "Neg logP: 5.387 (gt=2.290), params={'parameter_generator.core.weight': tensor([[ 5.6639],\n",
      "        [-6.5045]]), 'parameter_generator.core.bias': tensor([ 6.9974, -6.1344])}\n",
      "Neg logP: 5.334 (gt=2.267), params={'parameter_generator.core.weight': tensor([[ 5.6478],\n",
      "        [-6.2751]]), 'parameter_generator.core.bias': tensor([ 6.9950, -6.0778])}\n",
      "Neg logP: 5.280 (gt=2.261), params={'parameter_generator.core.weight': tensor([[ 5.6273],\n",
      "        [-6.0016]]), 'parameter_generator.core.bias': tensor([ 6.9920, -6.0108])}\n",
      "Neg logP: 5.245 (gt=2.296), params={'parameter_generator.core.weight': tensor([[ 5.6015],\n",
      "        [-5.6837]]), 'parameter_generator.core.bias': tensor([ 6.9881, -5.9329])}\n",
      "Neg logP: 5.151 (gt=2.257), params={'parameter_generator.core.weight': tensor([[ 5.5690],\n",
      "        [-5.3188]]), 'parameter_generator.core.bias': tensor([ 6.9831, -5.8434])}\n",
      "Neg logP: 5.043 (gt=2.212), params={'parameter_generator.core.weight': tensor([[ 5.5273],\n",
      "        [-4.8994]]), 'parameter_generator.core.bias': tensor([ 6.9766, -5.7417])}\n",
      "Neg logP: 5.042 (gt=2.290), params={'parameter_generator.core.weight': tensor([[ 5.4729],\n",
      "        [-4.4205]]), 'parameter_generator.core.bias': tensor([ 6.9680, -5.6262])}\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    x, y, true_logp = get_batch(100)\n",
    "    nlogp = -normal(y, cond=x).mean()\n",
    "    params = {k:v.detach() for k,v in normal.state_dict().items()}\n",
    "    if i % 100 == 0:\n",
    "        print(f'Neg logP: {nlogp:0.3f} (gt={-true_logp.mean():0.3f}), params={params}')\n",
    "    nlogp.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700ce5da-dd6b-41b4-8369-bb1414bf2b81",
   "metadata": {},
   "source": [
    "## One network with multiple outputs (returning positionally):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "012d4071-af9a-4486-90fc-8d27a6c4f38e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NormalParams(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.core = nn.Linear(1, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        vals = self.core(x)\n",
    "        #loc, scale = vals.split((1,1), dim=1)\n",
    "        return vals[:,0:1], (vals[:, 1:])**2\n",
    "        #return dict(loc=loc, scale=scale**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a8001caf-c7e7-4f01-a53d-d5d73046e32d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_batch(batch_size):\n",
    "    x = torch.rand((batch_size, 1))\n",
    "    mu = -5 * x + 9\n",
    "    scale = (3 * x + 1)\n",
    "    model = Normal(mu, scale=scale)\n",
    "    y = model.sample((batch_size,))\n",
    "    return x, y, model.log_prob(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "233f1a5f-086e-46c4-b512-bdf177a3f4f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make the mean & std both learnable\n",
    "# std (scale) is set to be square of a parameter, thus ensuring positive value\n",
    "normal = TrainableDistributionAdapter(Normal, _parameters=NormalParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f74d010-d81d-42fc-a405-fe327cfdbd1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optim = Adam(normal.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d378f8c8-bde5-409c-a1bf-765aa0fb98a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg logP: 3160.783 (gt=2.258), params={'parameter_generator.core.weight': tensor([[0.7723],\n",
      "        [0.6880]]), 'parameter_generator.core.bias': tensor([-0.7084, -0.8293])}\n",
      "Neg logP: 5.407 (gt=2.285), params={'parameter_generator.core.weight': tensor([[ 6.7786],\n",
      "        [-5.3161]]), 'parameter_generator.core.bias': tensor([ 5.3003, -6.8324])}\n",
      "Neg logP: 5.445 (gt=2.324), params={'parameter_generator.core.weight': tensor([[ 6.7787],\n",
      "        [-5.3145]]), 'parameter_generator.core.bias': tensor([ 5.3005, -6.8287])}\n",
      "Neg logP: 5.390 (gt=2.273), params={'parameter_generator.core.weight': tensor([[ 6.7784],\n",
      "        [-5.3119]]), 'parameter_generator.core.bias': tensor([ 5.3005, -6.8234])}\n",
      "Neg logP: 5.346 (gt=2.231), params={'parameter_generator.core.weight': tensor([[ 6.7780],\n",
      "        [-5.3087]]), 'parameter_generator.core.bias': tensor([ 5.3005, -6.8169])}\n",
      "Neg logP: 5.402 (gt=2.274), params={'parameter_generator.core.weight': tensor([[ 6.7776],\n",
      "        [-5.3051]]), 'parameter_generator.core.bias': tensor([ 5.3005, -6.8094])}\n",
      "Neg logP: 5.427 (gt=2.312), params={'parameter_generator.core.weight': tensor([[ 6.7771],\n",
      "        [-5.3010]]), 'parameter_generator.core.bias': tensor([ 5.3004, -6.8009])}\n",
      "Neg logP: 5.362 (gt=2.249), params={'parameter_generator.core.weight': tensor([[ 6.7766],\n",
      "        [-5.2964]]), 'parameter_generator.core.bias': tensor([ 5.3004, -6.7912])}\n",
      "Neg logP: 5.452 (gt=2.337), params={'parameter_generator.core.weight': tensor([[ 6.7760],\n",
      "        [-5.2912]]), 'parameter_generator.core.bias': tensor([ 5.3004, -6.7807])}\n",
      "Neg logP: 5.359 (gt=2.247), params={'parameter_generator.core.weight': tensor([[ 6.7753],\n",
      "        [-5.2855]]), 'parameter_generator.core.bias': tensor([ 5.3003, -6.7691])}\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    x, y, true_logp = get_batch(100)\n",
    "    nlogp = -normal(y, cond=x).mean()\n",
    "    params = {k:v.detach() for k,v in normal.state_dict().items()}\n",
    "    if i % 100 == 0:\n",
    "        print(f'Neg logP: {nlogp:0.3f} (gt={-true_logp.mean():0.3f}), params={params}')\n",
    "    nlogp.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf2da13-725c-4f64-af19-1432284474c0",
   "metadata": {},
   "source": [
    "# Test sampling and joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "11f3cc5a-da5f-4b28-8f33-ab7f44b38f62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prior = TrainableDistributionAdapter(Normal, loc=nn.Parameter(torch.Tensor([5])),\n",
    "                                     scale=torch.Tensor([2]))\n",
    "\n",
    "linear = nn.Linear(1, 1)\n",
    "linear.weight.data = torch.Tensor([[-2]])\n",
    "linear.bias.data = torch.Tensor([6])\n",
    "conditional = TrainableDistributionAdapter(Normal, loc=linear, scale=torch.Tensor([1]))\n",
    "\n",
    "# create a joint distribution out of prior and conditional\n",
    "joint = Joint(prior, conditional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1b33e734-1f33-4920-b997-864033282b11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x, y = joint.sample((10000,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5278584-1f7f-4741-a535-fb4115d17e24",
   "metadata": {},
   "source": [
    "Should be 5, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c4ae965-786b-4cd3-8e29-dfbe404c87b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4.9957), tensor(1.9761))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9ae6a0-b85f-4e96-87f1-8202de075983",
   "metadata": {},
   "source": [
    "Should be -4, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3440de7f-4bb6-4acb-9155-ca9abaa46e68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-3.9767), tensor(4.0760))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.mean(), y.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fcba96-c6d3-4d27-aaf0-322efa2c69c1",
   "metadata": {},
   "source": [
    "# Simulating a multi-dimensional joint distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a202a9f-482b-44a6-bc01-64674e729ec7",
   "metadata": {},
   "source": [
    "### Define the ground-truth generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "654699e9-7213-4aa2-81a0-cb264b8e1ad9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_latents = 5\n",
    "gt_prior = TrainableDistributionAdapter(MultivariateNormal, \n",
    "                                        torch.ones([n_latents]),\n",
    "                                        torch.eye(n_latents))\n",
    "\n",
    "n_obs = 10\n",
    "features = torch.randn([n_latents, n_obs])\n",
    "\n",
    "feature_map = nn.Linear(n_latents, n_obs)\n",
    "feature_map.weight.data = features.T\n",
    "feature_map.bias.data.zero_()\n",
    "\n",
    "gt_conditional = TrainableDistributionAdapter(MultivariateNormal,\n",
    "                                              feature_map,\n",
    "                                              torch.eye(n_obs))\n",
    "\n",
    "gt_joint = Joint(gt_prior, gt_conditional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670797ef-bada-4481-8084-660ec61fbc15",
   "metadata": {},
   "source": [
    "### Now prepare a trainable model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fd1342-4ac6-419a-aef6-b9c9e92a2d16",
   "metadata": {},
   "source": [
    "class Covariance(nn.Module):\n",
    "    def __init__(self, n_dim):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "04f84ea4-6f14-499a-a42a-a789a4ddcd1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_latents = 5\n",
    "model_prior = TrainableDistributionAdapter(MultivariateNormal, \n",
    "                                        nn.Parameter(torch.zeros([n_latents])),\n",
    "                                        PositiveDiagonal(n_latents))\n",
    "\n",
    "n_obs = 10\n",
    "features = torch.randn([n_latents, n_obs])\n",
    "\n",
    "feature_map = nn.Linear(n_latents, n_obs)\n",
    "#feature_map.weight.data = features.T\n",
    "#feature_map.bias.data.zero_()\n",
    "\n",
    "model_conditional = TrainableDistributionAdapter(MultivariateNormal,\n",
    "                                              feature_map,\n",
    "                                              torch.eye(n_obs))\n",
    "\n",
    "model_joint = Joint(model_prior, model_conditional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5aae5e-7c0f-4c05-be8e-a79c7b919d61",
   "metadata": {},
   "source": [
    "### Go ahead and train the joint model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e0aa86e-e17f-4e8d-ad65-9721841e0186",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optim = Adam(model_joint.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "92a32c09-c099-4593-b365-06f0a7f6000c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logp: -204.999 / GT logp: -21.587\n",
      "Model logp: -23.608 / GT logp: -21.388\n",
      "Model logp: -22.945 / GT logp: -21.200\n",
      "Model logp: -22.840 / GT logp: -21.416\n",
      "Model logp: -22.374 / GT logp: -21.205\n",
      "Model logp: -21.950 / GT logp: -20.999\n",
      "Model logp: -22.480 / GT logp: -21.573\n",
      "Model logp: -22.451 / GT logp: -21.343\n",
      "Model logp: -21.975 / GT logp: -21.006\n",
      "Model logp: -21.781 / GT logp: -21.408\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    samples = gt_joint.sample((100,))\n",
    "    gt_logl = gt_joint.log_prob(*samples).mean()\n",
    "    model_logl = model_joint.log_prob(*samples).mean()\n",
    "    if i % 100 == 0:\n",
    "        print(f'Model logp: {model_logl:.3f} / GT logp: {gt_logl:.3f}')\n",
    "    (-model_logl).backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23276263-918b-43d7-aca9-f18586b71b66",
   "metadata": {},
   "source": [
    "Checking the learned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2d55a492-57e4-4586-840f-f6619554a769",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8752, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 1.2800, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 1.0109, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.6556, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 1.0013]], grad_fn=<DiagBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_joint.prior._arg1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b647e339-4091-4a06-bbf8-07e326e6dd76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('_arg0', tensor([0.8684, 0.7502, 1.0640, 1.0752, 0.9583])),\n",
       "             ('_arg1.D',\n",
       "              tensor([ 0.9355,  1.1314, -1.0055, -0.8097, -1.0006]))])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_joint.prior.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d3b85c30-7306-4863-88d3-db6f66b8d720",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('_arg1',\n",
       "              tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "                      [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "                      [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "                      [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "                      [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "                      [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "                      [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       "             ('_arg0.weight',\n",
       "              tensor([[-1.3747, -0.5574,  0.0672, -0.4904,  0.2244],\n",
       "                      [-0.6747, -0.0906, -0.9939, -0.3891,  0.8754],\n",
       "                      [ 0.6459,  1.1428,  0.3236, -0.0247, -0.9512],\n",
       "                      [ 0.0231, -0.2420,  1.3344,  0.3168, -0.4036],\n",
       "                      [ 1.0081, -0.1965, -0.7567,  0.8509,  0.3718],\n",
       "                      [ 0.2194, -0.8537,  2.1729, -0.2634,  0.2488],\n",
       "                      [-2.1426, -0.0262, -1.0722, -0.4153, -1.0212],\n",
       "                      [-1.7564, -0.7689, -1.4497,  0.7013,  0.6128],\n",
       "                      [-0.4286, -0.5051, -1.2680,  1.1716,  0.7897],\n",
       "                      [-0.2391,  0.5127, -0.6762,  0.0155, -0.3593]])),\n",
       "             ('_arg0.bias',\n",
       "              tensor([ 0.1351,  0.1109, -0.1394, -0.1195, -0.0248, -0.1263, -0.0086,  0.0599,\n",
       "                       0.0959, -0.1777]))])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_joint.conditional.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1e676c-2beb-48f4-ba7a-16e6ddc4dfe6",
   "metadata": {},
   "source": [
    "## Compute the posterior distriubtion via ELBO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1721c353-3fc8-48a6-9dd2-3076db090b9b",
   "metadata": {},
   "source": [
    "First, we'll train the posterior for the ground-truth model, using ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e8a1d168-d44e-4981-9377-44d8eadb6aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare a posterior distribution\n",
    "features = torch.randn([n_obs, n_latents])\n",
    "\n",
    "linear_map = nn.Linear(n_obs, n_latents)\n",
    "\n",
    "posterior = TrainableDistributionAdapter(MultivariateNormal,\n",
    "                                              linear_map,\n",
    "                                              Covariance(n_latents, rank=5, eps=1e-6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f692ccc5-cbbe-4c2c-bea3-ba29d6f0e9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo_x = ELBOMarginal(gt_joint, posterior)\n",
    "\n",
    "# only training the posterior\n",
    "optim = Adam(posterior.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9b3ead08-3a46-495b-af06-7b56d838b713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model elbo: -726.289\n",
      "Model elbo: -42.695\n",
      "Model elbo: -24.010\n",
      "Model elbo: -20.559\n",
      "Model elbo: -20.051\n",
      "Model elbo: -19.918\n",
      "Model elbo: -19.868\n",
      "Model elbo: -19.838\n",
      "Model elbo: -19.816\n",
      "Model elbo: -19.827\n"
     ]
    }
   ],
   "source": [
    "z_sample, x_sample = gt_joint.sample((1000,))\n",
    "\n",
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    elbo = elbo_x(x_sample).mean()\n",
    "    if i % 100 == 0:\n",
    "        print(f'Model elbo: {elbo:.3f}')\n",
    "    (-elbo).backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76d032b-2ed2-4e09-a803-69f0463a4e0b",
   "metadata": {},
   "source": [
    "### Evaluate the posterior\n",
    "Here we will evaluate how good the posterior is (roughly) by sampling $\\hat{z}$ from the posterior $p(z|x)$ and evaluate the expected $\\log p(\\hat{z}, x)$.\n",
    "If $\\hat{z}$ approximates the true distribution over $z$, then $\\log p(\\hat{z}, x)$ will closely apprximate the expected $\\log p(z, x)$, which is negative of entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8581b914-7f75-4381-b3ec-d9acb6d486d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_sample, x_sample = gt_joint.sample((1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "07fea8fa-e733-4c3c-8a68-efeb80c95dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-21.3407, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ground-truth negative entropy\n",
    "gt_joint.log_prob(z_sample, x_sample).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5f49637e-67c9-4107-a824-7f56e93ac040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-21.3697, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample from the trained posterior\n",
    "z_hat = posterior.sample(cond=x_sample)\n",
    "\n",
    "# \n",
    "# negative entropy of the approximation\n",
    "gt_joint.log_prob(z_hat, x_sample).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b38dcd-8b0d-421f-a5ba-d70ff18656b0",
   "metadata": {},
   "source": [
    "## Compute the posterior distriubtion via direct fit to samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1684cb3d-c162-4308-8c09-81cb7caf746f",
   "metadata": {},
   "source": [
    "Now, we'll train the posterior for the ground-truth model by training directly on the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "21e531b0-2329-4c10-b025-d9821af8ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a posterior distribution\n",
    "features = torch.randn([n_obs, n_latents])\n",
    "\n",
    "linear_map = nn.Linear(n_obs, n_latents)\n",
    "\n",
    "posterior = TrainableDistributionAdapter(MultivariateNormal,\n",
    "                                              linear_map,\n",
    "                                              Covariance(n_latents, rank=5, eps=1e-6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d24a758e-6a95-4972-afa5-9501a301e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only training the posterior\n",
    "optim = Adam(posterior.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f093b35a-640d-4199-bbdc-b94d850a73d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogP: -142.656\n",
      "LogP: -7.445\n",
      "LogP: -6.255\n",
      "LogP: -4.870\n",
      "LogP: -4.178\n",
      "LogP: -3.914\n",
      "LogP: -3.563\n",
      "LogP: -3.272\n",
      "LogP: -3.157\n",
      "LogP: -2.958\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    z_sample, x_sample = gt_joint.sample((100,))\n",
    "    logp = posterior.log_prob(z_sample, cond=x_sample).mean()\n",
    "    if i % 100 == 0:\n",
    "        print(f'LogP: {logp:.3f}')\n",
    "    (-logp).backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105cf468-c7a1-4520-b0b8-6b88073a1f67",
   "metadata": {},
   "source": [
    "### Evaluate the posterior\n",
    "Here we will evaluate how good the posterior is (roughly) by sampling $\\hat{z}$ from the posterior $p(z|x)$ and evaluate the expected $\\log p(\\hat{z}, x)$.\n",
    "If $\\hat{z}$ approximates the true distribution over $z$, then $\\log p(\\hat{z}, x)$ will closely apprximate the expected $\\log p(z, x)$, which is negative of entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4ab93336-46a3-4eff-8ae3-8e2e5bf8f517",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_sample, x_sample = gt_joint.sample((1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "017c8ad0-0633-4ee9-aba0-431bbd096073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-21.2337, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ground-truth negative entropy\n",
    "gt_joint.log_prob(z_sample, x_sample).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8c51f065-1653-454c-b27f-b88b109b86eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-37.6097, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample from the trained posterior\n",
    "z_hat = posterior.sample(cond=x_sample)\n",
    "\n",
    "# \n",
    "# negative entropy of the approximation\n",
    "gt_joint.log_prob(z_hat, x_sample).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
