{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8a632de-d9e7-4ea5-ba54-6542235f5a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Normal, MultivariateNormal\n",
    "from torch.optim import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8359f593-177c-443d-9411-23abc8276b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: overall, the handling of returned tuples for the samples is very loose\n",
    "# This stems primarily from the rather inconsitent interface/handling of random variables\n",
    "# of the distribution (can be a singleton/non-tuple OR tuple) vs random variables\n",
    "# you are conditioned on (generally expected to be a tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c59a8a-9574-4bea-be63-b6fce61f8068",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def register_to_module(module, field, value):\n",
    "    if isinstance(value, torch.Tensor) and not isinstance(value, nn.Parameter):\n",
    "        # register as buffer\n",
    "        module.register_buffer(field, value)\n",
    "    else:\n",
    "        setattr(module, field, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16708a8-4193-4849-8ee3-b8bc841d7535",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class TrainableDistributionAdapter(nn.Module):\n",
    "    \n",
    "    n_rvs = 1\n",
    "    \n",
    "    def __init__(self, distribution_class, *dist_args, _parameters=None, **dist_kwargs):\n",
    "        super().__init__()\n",
    "        self.distrbrituion_class = distribution_class\n",
    "        self.param_counts = len(dist_args)\n",
    "        self.param_keys = list(dist_kwargs.keys())\n",
    "        \n",
    "        for pos, val in enumerate(dist_args):\n",
    "            #setattr(self, f'_arg{pos}', val)\n",
    "            register_to_module(self, f'_arg{pos}', val)\n",
    "        for key, val in dist_kwargs.items():\n",
    "            register_to_module(self, key, val)\n",
    "            \n",
    "        # The alternative interface to allow for a single module to \n",
    "        # flexibly output multiple parameters for the distribution\n",
    "        # at the moment, the module is expected to output a dictionary\n",
    "        # of parameters\n",
    "        if _parameters is not None:\n",
    "            self.parameter_generator = _parameters\n",
    "            \n",
    "    \n",
    "    def distribution(self, cond=None):\n",
    "        cond = turn_to_tuple(cond)\n",
    "        \n",
    "        # a helper function to visit the target field\n",
    "        # and invoke the field with cond if it is a nn.Module.\n",
    "        # Otherwise, simply return the field content\n",
    "        def parse_attr(field, cond=None):\n",
    "            attr = getattr(self, field)\n",
    "            if isinstance(attr, nn.Module):\n",
    "                attr = attr(*cond)\n",
    "            return attr\n",
    "        \n",
    "        dist_args = tuple(parse_attr(f'_arg{pos}', cond=cond) for pos in range(self.param_counts))\n",
    "        dist_kwargs = {k: parse_attr(k, cond=cond) for k in self.param_keys}\n",
    "\n",
    "        # TODO: consider flipping the order of this with\n",
    "        # init specified parameters\n",
    "        if hasattr(self, 'parameter_generator'):\n",
    "            dist_args, dist_kwargs = make_args(self.parameter_generator(*cond), *dist_args, **dist_kwargs)\n",
    "            \n",
    "        return self.distrbrituion_class(*dist_args, **dist_kwargs)\n",
    "    \n",
    "    def log_prob(self, *obs, cond=None):\n",
    "        return self.distribution(cond=cond).log_prob(*obs)\n",
    "    \n",
    "    def forward(self, *obs, cond=None):\n",
    "        return self.log_prob(*obs, cond=cond)\n",
    "    \n",
    "    def sample(self, sample_shape=torch.Size([]), cond=None):\n",
    "        return self.distribution(cond=cond).sample(sample_shape=sample_shape)\n",
    "    \n",
    "    def rsample(self, sample_shape=torch.Size([]), cond=None):\n",
    "        return self.distribution(cond=cond).rsample(sample_shape=sample_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c2a4e9-9498-4a3d-9f70-721ec910b811",
   "metadata": {},
   "source": [
    "y = f(x)\n",
    "\n",
    "p(y) =  det dx/dy p(x)\n",
    "\n",
    "p(y) = 1 / f'(x) p(x)\n",
    "\n",
    "p(f(x)) * f'(x) = p(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66575a89-89b3-42fe-b1c6-4ad4efe1e867",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# come up with a better name\n",
    "# here we assume that conditioning of the joint distribution can occur\n",
    "# by conditioning the prior\n",
    "class Joint(nn.Module):\n",
    "    def __init__(self, prior, conditional):\n",
    "        super().__init__()\n",
    "        self.prior = prior\n",
    "        self.conditional = conditional\n",
    "        self.split = self.prior.n_rvs\n",
    "        self.n_rvs = self.prior.n_rvs + self.conditional.n_rvs\n",
    "        \n",
    "    def log_prob(self, *obs, cond=None):\n",
    "        # TODO: maybe just use self.prior.n_rvs\n",
    "        x, y = obs[:self.split], obs[self.split:]\n",
    "        return self.prior(*x, cond=cond) + self.conditional(*y, cond=x)\n",
    "    \n",
    "    def forward(self, *obs, cond=None):\n",
    "        return self.log_prob(*obs, cond=cond)\n",
    "    \n",
    "    def sample(self, sample_shape=torch.Size([]), cond=None):\n",
    "        x_samples = self.prior.sample(sample_shape=sample_shape, cond=cond)\n",
    "        y_samples = self.conditional.sample(cond=x_samples)\n",
    "        return turn_to_tuple(x_samples) + turn_to_tuple(y_samples)\n",
    "    \n",
    "    def rsample(self, sample_shape=torch.Size([]), cond=None):\n",
    "        x_samples = self.prior.rsample(sample_shape=sample_shape, cond=cond)\n",
    "        y_samples = self.conditional.rsample(cond=x_samples)\n",
    "        return turn_to_tuple(x_samples) + turn_to_tuple(y_samples)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bba839-d7ce-4670-9398-47acfbf414ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# conceptual template\n",
    "class InvertibleTransform(nn.Module):\n",
    "    def forward(self, x, logL=0):\n",
    "        return y, logL + log_det_f_prime\n",
    "    \n",
    "    def reverse(self, y, logL=0):\n",
    "        return x, logL - log_det_f_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fe6f7e-b4ca-4e96-aa23-a8ebb972d650",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class SequentialTransform(nn.Module):\n",
    "    def __init__(self, *transforms):\n",
    "        self.transforms = nn.ModuleList(transforms)\n",
    "        \n",
    "    # TODO: check the content of nn.Module __call__ and create something similar for reverse\n",
    "    def forward(self, x, logL=0, cond=None):\n",
    "        for t in self.transforms:\n",
    "            x, logL = t(x, logL, cond=cond)\n",
    "        return x, logL\n",
    "    \n",
    "    def reverse(self, y, logL=0, cond=None):\n",
    "        for t in self.transforms[::-1]:\n",
    "            y, logL = t(y, logL, cond=cond)\n",
    "        return y, logL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382e1805-5130-4e16-8508-8e19a216452a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class FlowDistribution(nn.Module):\n",
    "    def __init__(self, base_distribution, transform):\n",
    "        super().__init__()\n",
    "        self.base_distribution = base_distribution\n",
    "        self.transform = transform\n",
    "        \n",
    "    def forward(self, *obs, cond=None):\n",
    "        return self.log_prob(*obs, cond=cond)\n",
    "        \n",
    "    def log_prob(self, *obs, cond=None):\n",
    "        x, logL = self.transform.reverse(*obs, cond=cond)\n",
    "        return self.base_distribution.log_prob(*turn_to_tuple(x), cond=cond) + logL\n",
    "    \n",
    "    def sample(self, sample_shape=torch.Size([]), cond=None):\n",
    "        samples = self.base_distribution.sample(sample_shape=sample_shape, cond=cond)\n",
    "        y, _ = self.transform(samples, cond=cond)\n",
    "        return y\n",
    "    \n",
    "    def rsample(self, sample_shape=torch.Size([]), cond=None):\n",
    "        samples = self.base_distribution.rsample(sample_shape=sample_shape, cond=cond)\n",
    "        y, _ = self.transform(samples, cond=cond)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe51d144-49cc-418c-b290-4094dd241775",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def ELBO_joint(joint, posterior, *obs, n_samples=1):\n",
    "    # Joint = p(z, x), Posterior = p(z|x)\n",
    "    z_samples = posterior.rsample((n_samples,), cond=obs)\n",
    "    # take care of case where KL is known for the posterior\n",
    "    elbo = -posterior(*turn_to_tuple(z_samples), cond=obs)\n",
    "    elbo += joint(*turn_to_tuple(z_samples), *obs)\n",
    "    return elbo\n",
    "\n",
    "def ELBO_parts(prior, conditional, posterior, *obs, n_samples=1):\n",
    "    # create a joint\n",
    "    joint = Joint(prior, conditional)\n",
    "    return ELBO_joint(joint, posterior, *obs, n_samples=n_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad619d0-df83-4e4a-bd9e-14751e7cba45",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class ELBOMarginal(nn.Module):\n",
    "    def __init__(self, joint, posterior, n_samples=1):\n",
    "        super().__init__()\n",
    "        self.joint = joint\n",
    "        self.posterior = posterior\n",
    "        # infer how many variables are in observations\n",
    "        self.n_rvs = joint.n_rvs - posterior.n_rvs\n",
    "        self.n_samples = n_samples\n",
    "        \n",
    "    def forward(self, *obs, cond=None):\n",
    "        return self.elbo(*obs, cond=cond)\n",
    "        \n",
    "    def elbo(self, *obs, cond=None):\n",
    "        # TODO: deal with conditioning correctly\n",
    "        return ELBO_joint(self.joint, self.posterior, *obs, n_samples=self.n_samples)\n",
    "    \n",
    "    def log_prob(self, *obs):\n",
    "        # TODO: let this be implemented as an \"approximation\" with ELBO\n",
    "        # but with ample warnings\n",
    "        pass\n",
    "    \n",
    "    def sample(self, sample_shape=torch.Size([]), cond=None):\n",
    "        samples = self.joint.sample(sample_shape=sample_shape, cond=cond)\n",
    "        return samples[-self.n_rvs:]\n",
    "    \n",
    "    def rsample(self, sample_shape=torch.Size([]), cond=None):\n",
    "        samples = self.joint.rsample(sample_shape=sample_shape, cond=cond)\n",
    "        return samples[-self.n_rvs:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7499c6-6070-474f-a70a-cfe6e12db41a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#TODO Consider implementing SurVAE\n",
    "class SurVAE(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b4867e-68e5-4657-b45b-950d977c615e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def turn_to_tuple(x):\n",
    "    \"\"\"\n",
    "    Given a value x, turn into a consistent tuple\n",
    "    * if x is None, return an empty tumple ()\n",
    "    * if x is a non-tuple value, return as a single-element tuple (x,)\n",
    "    * if x is already a tuple\n",
    "    \"\"\"\n",
    "    if x is None:\n",
    "        return ()\n",
    "    return x if isinstance(x, tuple) else (x,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ace806c-46af-4b96-82f4-7a53206c69b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from warnings import warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf549bf2-5b98-46a8-900e-96b17e8dc41a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def make_args(x, *args, **kwargs):\n",
    "    if isinstance(x, dict): # TODO: consider making it a Collection.Mapping\n",
    "        kwargs.update(x)\n",
    "    elif isinstance(x, tuple):\n",
    "        args = x + args\n",
    "    else:\n",
    "        args = (x,) + args\n",
    "        \n",
    "    return args, kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2026682a-166f-4dda-9ecb-686aa54d78e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class TransformedParameter(nn.Module):\n",
    "    def __init__(self, tensor, transform_fn=None):\n",
    "        super().__init__()\n",
    "        self.parameter = nn.Parameter(tensor)\n",
    "        if transform_fn is None:\n",
    "            transform_fn = lambda x: x\n",
    "        self.transform_fn = transform_fn\n",
    "        \n",
    "    @property\n",
    "    def value(self):\n",
    "        return self()\n",
    "        \n",
    "    def forward(self, *args):\n",
    "        return self.transform_fn(self.parameter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4c241b-bafd-4663-b96e-3a8d301fd351",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class Covariance(nn.Module):\n",
    "    def __init__(self, n_dims, rank=None, eps=1e-16):\n",
    "        super().__init__()\n",
    "        if rank is None:\n",
    "            rank = n_dims\n",
    "        self.n_dims = n_dims\n",
    "        self.rank = rank\n",
    "        self.eps = eps\n",
    "        self.A = nn.Parameter(torch.randn(n_dims, rank))\n",
    "        \n",
    "    def forward(self, *args):\n",
    "        return self.A @ self.A.T + torch.eye(self.n_dims) * self.eps\n",
    "    \n",
    "    @property\n",
    "    def value(self):\n",
    "        return self()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b4dbd2-daf6-4133-92a5-fe873e9617ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# TODO: generalize this so that positiveness can arise from other functions\n",
    "class PositiveDiagonal(nn.Module):\n",
    "    def __init__(self, n_dims, eps=1e-16):\n",
    "        super().__init__()\n",
    "        self.n_dims = n_dims\n",
    "        self.eps = eps\n",
    "        self.D = nn.Parameter(torch.randn(n_dims))\n",
    "        \n",
    "    def forward(self, *args):\n",
    "        return torch.diag(self.D**2 + self.eps)\n",
    "    \n",
    "    @property\n",
    "    def value(self):\n",
    "        return self()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9559a6a1-d356-48ae-b0df-5cd133614709",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class ProbabilisticSIModel(nn.Module):\n",
    "    # TODO: for this to work, the TrainableDistributionAdapter handling of\n",
    "    # _parameters must be expanded. Namely, it needs to be able to accept:\n",
    "    # * positional arguments (to be implemented)\n",
    "    # * dict (already implemented)\n",
    "    # * dict with positional arguments (to be implemented)\n",
    "    # \n",
    "    # To also make this generically useful, it would be helpful to\n",
    "    # allow for output conversion function to be supplied. This function then\n",
    "    # should transform outputs of the SI model into format appropriate\n",
    "    # to serve as _parameters for the TrainableDistributionAdapter\n",
    "    # It's important that such transformation does NOT warp the output\n",
    "    # Doing so will distort the probability density!\n",
    "    def __init__(self, si_model, distribution_class, *dist_args, **dist_kwargs):\n",
    "        super().__init__()\n",
    "        sielf.si_model = si_model\n",
    "        self.distribution_class = distribution_class\n",
    "        self.trainable_distribution = TrainableDistributionAdapter(distribution_class, *dist_args,\n",
    "                                                                   _parameters=si_model, **dist_kwargs)\n",
    "        \n",
    "    def log_prob(self, *obs, cond=None):\n",
    "        return self.trainable_distribution.log_prob(*obs, cond=cond)\n",
    "    \n",
    "    def forward(self, *obs, cond=None):\n",
    "        return self.log_prob(*obs, cond=cond)\n",
    "    \n",
    "    def sample(self, sample_shape=torch.Size([]), cond=None):\n",
    "        return self.trainable_distribution.sample(sample_shape=sample_shape, cond=cond)\n",
    "\n",
    "    def rsample(self, sample_shape=torch.Size([]), cond=None):\n",
    "        return self.trainable_distribution.rsample(sample_shape=sample_shape, cond=cond)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da3a3f0-6496-44c2-877a-51e43049d564",
   "metadata": {},
   "source": [
    "## Example usage: learn Normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82fc8a0-3d0a-4787-8f7e-04bc9b7e4ad5",
   "metadata": {},
   "source": [
    "### Learn only the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342a96b1-29ef-43ea-bc49-9eb2ad4c2792",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# make the mean & std both learnable\n",
    "# std (scale) is set to be square of a parameter, thus ensuring positive value\n",
    "normal = TrainableDistributionAdapter(Normal, \n",
    "                               loc=nn.Parameter(torch.Tensor([0.0])), \n",
    "                               scale=torch.Tensor([2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f047e26-9865-439e-960c-35d10cf87447",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "normal.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdde714-cb26-4019-9a1e-3ed68ad8528b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "list(normal.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75b75fc-f810-4ff1-b345-f41665612649",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# setup target normal distribution to learn\n",
    "target = Normal(torch.Tensor([5.0]), torch.Tensor([2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb2a510-507b-4bb9-8f9b-11a3d1b27b06",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "optim = Adam(normal.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429c9425-d8bc-488d-b002-ac106a04aac6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    targets = target.sample((100,))\n",
    "    nlogp = -normal(targets).mean()\n",
    "    mean = normal.loc.detach()\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f'Neg logP: {nlogp}, mean={mean}')\n",
    "    nlogp.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439fe712-7fa3-4a7a-b65c-b21e0332e2b9",
   "metadata": {},
   "source": [
    "### Learn both mean and stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7809263a-c323-4fe2-bcf1-d9d0ef714c4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# make the mean & std both learnable\n",
    "# std (scale) is set to be square of a parameter, thus ensuring positive value\n",
    "normal = TrainableDistributionAdapter(Normal, \n",
    "                               loc=nn.Parameter(torch.Tensor([0.0])), \n",
    "                               scale=TransformedParameter(torch.Tensor([1.0]), lambda x: x**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079c5d35-b3b6-41c6-b880-20f05a75f253",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# setup target normal distribution to learn\n",
    "target = Normal(torch.Tensor([5]), torch.Tensor([8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe37983e-40d5-4932-8530-7965a4938788",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "optim = Adam(normal.parameters(), lr=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd2836c-e6f0-416f-a851-3bdbd168b0fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    targets = target.sample((100,))\n",
    "    nlogp = -normal(targets).mean()\n",
    "    mean = normal.loc.detach()\n",
    "    std = normal.scale.value.detach()\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f'Neg logP: {nlogp}, mean={mean}, std={std}')\n",
    "    nlogp.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ebdfb9-b71b-4131-bff4-afe0cc963aba",
   "metadata": {},
   "source": [
    "### Learn both mean and stdev (same as above, but specified positionally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fd10a4-5e15-479a-8cdf-d79e48014746",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# make the mean & std both learnable\n",
    "# std (scale) is set to be square of a parameter, thus ensuring positive value\n",
    "normal = TrainableDistributionAdapter(Normal, \n",
    "                               nn.Parameter(torch.Tensor([0.0])), \n",
    "                               TransformedParameter(torch.Tensor([1.0]), lambda x: x**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bcbc8b-ae5a-409e-8189-3fc127626849",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# setup target normal distribution to learn\n",
    "target = Normal(torch.Tensor([5]), torch.Tensor([8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a92ccf-bc7a-46d8-ab42-83118e6803be",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "optim = Adam(normal.parameters(), lr=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0725d790-8b17-4e59-b18b-bd500827adfe",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    targets = target.sample((100,))\n",
    "    nlogp = -normal(targets).mean()\n",
    "    mean = normal._arg0.detach()\n",
    "    std = normal._arg1.value.detach()\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f'Neg logP: {nlogp}, mean={mean}, std={std}')\n",
    "    nlogp.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057437e7-add2-4d09-99ff-3711dd667f3f",
   "metadata": {},
   "source": [
    "# Conditional case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf10fc1f-e412-410e-abcb-435a4dfd4342",
   "metadata": {},
   "source": [
    "## Simple conditioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c02a881-565f-4acf-a07b-010e0031bc89",
   "metadata": {},
   "source": [
    "Now let us learn more complex relationship $p(z|x)$. Specifically, let $p(z|x) = \\mathcal{N}(f(x), \\sigma^2)$.\n",
    "\n",
    "For simplicity, we'll assume a simple linear mapping for $f(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a03c77-6e88-447a-b848-694c05115cfd",
   "metadata": {},
   "source": [
    "Prepare data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d527ba6-0b3f-45da-8d53-d66961bcd9a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def get_batch(batch_size):\n",
    "    x = torch.rand((batch_size, 1))\n",
    "    mu = -5 * x + 9\n",
    "    y = Normal(mu, scale=7).sample((batch_size,))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60912ec7-5621-4009-a984-a6da7c464633",
   "metadata": {},
   "source": [
    "Set up the conditional network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e054cdc5-a040-46eb-893a-c195ed186b3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# make the mean & std both learnable\n",
    "# std (scale) is set to be abs of a parameter, thus ensuring positive value\n",
    "normal = TrainableDistributionAdapter(Normal, \n",
    "                               loc=nn.Linear(1, 1), \n",
    "                               scale=TransformedParameter(torch.Tensor([1.0]), torch.abs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73da766-cdc7-4489-8f12-94f5889a71dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "optim = Adam(normal.parameters(), lr=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6c9324-239c-4b22-896c-3438d041e7d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    x, y = get_batch(100)\n",
    "    nlogp = -normal(y, cond=x).mean()\n",
    "    params = {k:v.detach() for k,v in normal.state_dict().items()}\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f'Neg logP: {nlogp}, params={params}')\n",
    "    nlogp.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b214cea-c48a-48eb-b416-a70c8450c351",
   "metadata": {},
   "source": [
    "## One network with multiple outputs (returning dict):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4583b2d-4a12-475d-9c1c-0dc5dd6be5f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class NormalParams(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.core = nn.Linear(1, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        vals = self.core(x)\n",
    "        #loc, scale = vals.split((1,1), dim=1)\n",
    "        return dict(loc=vals[:,0:1], scale=(vals[:, 1:])**2)\n",
    "        #return dict(loc=loc, scale=scale**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02028ecf-19e7-47bb-8ac5-70bec962af6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def get_batch(batch_size):\n",
    "    x = torch.rand((batch_size, 1))\n",
    "    mu = -5 * x + 9\n",
    "    scale = (3 * x + 1)\n",
    "    model = Normal(mu, scale=scale)\n",
    "    y = model.sample((batch_size,))\n",
    "    return x, y, model.log_prob(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445ebdae-679a-4ae7-ba4a-250732bd645b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# make the mean & std both learnable\n",
    "# std (scale) is set to be square of a parameter, thus ensuring positive value\n",
    "normal = TrainableDistributionAdapter(Normal, _parameters=NormalParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a3c3e8-d032-4c6a-98fc-b5aaf6e1ca6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "optim = Adam(normal.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab641be3-668e-434e-8c7b-c39bbee50871",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    x, y, true_logp = get_batch(100)\n",
    "    nlogp = -normal(y, cond=x).mean()\n",
    "    params = {k:v.detach() for k,v in normal.state_dict().items()}\n",
    "    if i % 100 == 0:\n",
    "        print(f'Neg logP: {nlogp:0.3f} (gt={-true_logp.mean():0.3f}), params={params}')\n",
    "    nlogp.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700ce5da-dd6b-41b4-8369-bb1414bf2b81",
   "metadata": {},
   "source": [
    "## One network with multiple outputs (returning positionally):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012d4071-af9a-4486-90fc-8d27a6c4f38e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class NormalParams(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.core = nn.Linear(1, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        vals = self.core(x)\n",
    "        #loc, scale = vals.split((1,1), dim=1)\n",
    "        return vals[:,0:1], (vals[:, 1:])**2\n",
    "        #return dict(loc=loc, scale=scale**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8001caf-c7e7-4f01-a53d-d5d73046e32d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def get_batch(batch_size):\n",
    "    x = torch.rand((batch_size, 1))\n",
    "    mu = -5 * x + 9\n",
    "    scale = (3 * x + 1)\n",
    "    model = Normal(mu, scale=scale)\n",
    "    y = model.sample((batch_size,))\n",
    "    return x, y, model.log_prob(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233f1a5f-086e-46c4-b512-bdf177a3f4f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# make the mean & std both learnable\n",
    "# std (scale) is set to be square of a parameter, thus ensuring positive value\n",
    "normal = TrainableDistributionAdapter(Normal, _parameters=NormalParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f74d010-d81d-42fc-a405-fe327cfdbd1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "optim = Adam(normal.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d378f8c8-bde5-409c-a1bf-765aa0fb98a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    x, y, true_logp = get_batch(100)\n",
    "    nlogp = -normal(y, cond=x).mean()\n",
    "    params = {k:v.detach() for k,v in normal.state_dict().items()}\n",
    "    if i % 100 == 0:\n",
    "        print(f'Neg logP: {nlogp:0.3f} (gt={-true_logp.mean():0.3f}), params={params}')\n",
    "    nlogp.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf2da13-725c-4f64-af19-1432284474c0",
   "metadata": {},
   "source": [
    "# Test sampling and joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f3cc5a-da5f-4b28-8f33-ab7f44b38f62",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "prior = TrainableDistributionAdapter(Normal, loc=nn.Parameter(torch.Tensor([5])),\n",
    "                                     scale=torch.Tensor([2]))\n",
    "\n",
    "linear = nn.Linear(1, 1)\n",
    "linear.weight.data = torch.Tensor([[-2]])\n",
    "linear.bias.data = torch.Tensor([6])\n",
    "conditional = TrainableDistributionAdapter(Normal, loc=linear, scale=torch.Tensor([1]))\n",
    "\n",
    "# create a joint distribution out of prior and conditional\n",
    "joint = Joint(prior, conditional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b33e734-1f33-4920-b997-864033282b11",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "x, y = joint.sample((10000,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5278584-1f7f-4741-a535-fb4115d17e24",
   "metadata": {},
   "source": [
    "Should be 5, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4ae965-786b-4cd3-8e29-dfbe404c87b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9ae6a0-b85f-4e96-87f1-8202de075983",
   "metadata": {},
   "source": [
    "Should be -4, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3440de7f-4bb6-4acb-9155-ca9abaa46e68",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "y.mean(), y.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fcba96-c6d3-4d27-aaf0-322efa2c69c1",
   "metadata": {},
   "source": [
    "# Simulating a multi-dimensional joint distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a202a9f-482b-44a6-bc01-64674e729ec7",
   "metadata": {},
   "source": [
    "### Define the ground-truth generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654699e9-7213-4aa2-81a0-cb264b8e1ad9",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "n_latents = 5\n",
    "gt_prior = TrainableDistributionAdapter(MultivariateNormal, \n",
    "                                        torch.ones([n_latents]),\n",
    "                                        torch.eye(n_latents))\n",
    "\n",
    "n_obs = 10\n",
    "features = torch.randn([n_latents, n_obs])\n",
    "\n",
    "feature_map = nn.Linear(n_latents, n_obs)\n",
    "feature_map.weight.data = features.T\n",
    "feature_map.bias.data.zero_()\n",
    "\n",
    "gt_conditional = TrainableDistributionAdapter(MultivariateNormal,\n",
    "                                              feature_map,\n",
    "                                              torch.eye(n_obs))\n",
    "\n",
    "gt_joint = Joint(gt_prior, gt_conditional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670797ef-bada-4481-8084-660ec61fbc15",
   "metadata": {},
   "source": [
    "### Now prepare a trainable model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fd1342-4ac6-419a-aef6-b9c9e92a2d16",
   "metadata": {},
   "source": [
    "class Covariance(nn.Module):\n",
    "    def __init__(self, n_dim):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f84ea4-6f14-499a-a42a-a789a4ddcd1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "n_latents = 5\n",
    "model_prior = TrainableDistributionAdapter(MultivariateNormal, \n",
    "                                        nn.Parameter(torch.zeros([n_latents])),\n",
    "                                        PositiveDiagonal(n_latents))\n",
    "\n",
    "n_obs = 10\n",
    "features = torch.randn([n_latents, n_obs])\n",
    "\n",
    "feature_map = nn.Linear(n_latents, n_obs)\n",
    "#feature_map.weight.data = features.T\n",
    "#feature_map.bias.data.zero_()\n",
    "\n",
    "model_conditional = TrainableDistributionAdapter(MultivariateNormal,\n",
    "                                              feature_map,\n",
    "                                              torch.eye(n_obs))\n",
    "\n",
    "model_joint = Joint(model_prior, model_conditional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5aae5e-7c0f-4c05-be8e-a79c7b919d61",
   "metadata": {},
   "source": [
    "### Go ahead and train the joint model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0aa86e-e17f-4e8d-ad65-9721841e0186",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "optim = Adam(model_joint.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a32c09-c099-4593-b365-06f0a7f6000c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    samples = gt_joint.sample((100,))\n",
    "    gt_logl = gt_joint.log_prob(*samples).mean()\n",
    "    model_logl = model_joint.log_prob(*samples).mean()\n",
    "    if i % 100 == 0:\n",
    "        print(f'Model logp: {model_logl:.3f} / GT logp: {gt_logl:.3f}')\n",
    "    (-model_logl).backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23276263-918b-43d7-aca9-f18586b71b66",
   "metadata": {},
   "source": [
    "Checking the learned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d55a492-57e4-4586-840f-f6619554a769",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model_joint.prior._arg1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b647e339-4091-4a06-bbf8-07e326e6dd76",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model_joint.prior.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b85c30-7306-4863-88d3-db6f66b8d720",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model_joint.conditional.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1e676c-2beb-48f4-ba7a-16e6ddc4dfe6",
   "metadata": {},
   "source": [
    "## Compute the posterior distriubtion via ELBO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1721c353-3fc8-48a6-9dd2-3076db090b9b",
   "metadata": {},
   "source": [
    "First, we'll train the posterior for the ground-truth model, using ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a1d168-d44e-4981-9377-44d8eadb6aaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# prepare a posterior distribution\n",
    "features = torch.randn([n_obs, n_latents])\n",
    "\n",
    "linear_map = nn.Linear(n_obs, n_latents)\n",
    "\n",
    "posterior = TrainableDistributionAdapter(MultivariateNormal,\n",
    "                                              linear_map,\n",
    "                                              Covariance(n_latents, rank=5, eps=1e-6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f692ccc5-cbbe-4c2c-bea3-ba29d6f0e9b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "elbo_x = ELBOMarginal(gt_joint, posterior)\n",
    "\n",
    "# only training the posterior\n",
    "optim = Adam(posterior.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3ead08-3a46-495b-af06-7b56d838b713",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "z_sample, x_sample = gt_joint.sample((1000,))\n",
    "\n",
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    elbo = elbo_x(x_sample).mean()\n",
    "    if i % 100 == 0:\n",
    "        print(f'Model elbo: {elbo:.3f}')\n",
    "    (-elbo).backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76d032b-2ed2-4e09-a803-69f0463a4e0b",
   "metadata": {},
   "source": [
    "### Evaluate the posterior\n",
    "Here we will evaluate how good the posterior is (roughly) by sampling $\\hat{z}$ from the posterior $p(z|x)$ and evaluate the expected $\\log p(\\hat{z}, x)$.\n",
    "If $\\hat{z}$ approximates the true distribution over $z$, then $\\log p(\\hat{z}, x)$ will closely apprximate the expected $\\log p(z, x)$, which is negative of entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8581b914-7f75-4381-b3ec-d9acb6d486d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "z_sample, x_sample = gt_joint.sample((1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fea8fa-e733-4c3c-8a68-efeb80c95dae",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ground-truth negative entropy\n",
    "gt_joint.log_prob(z_sample, x_sample).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f49637e-67c9-4107-a824-7f56e93ac040",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# sample from the trained posterior\n",
    "z_hat = posterior.sample(cond=x_sample)\n",
    "\n",
    "# \n",
    "# negative entropy of the approximation\n",
    "gt_joint.log_prob(z_hat, x_sample).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b38dcd-8b0d-421f-a5ba-d70ff18656b0",
   "metadata": {},
   "source": [
    "## Compute the posterior distriubtion via direct fit to samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1684cb3d-c162-4308-8c09-81cb7caf746f",
   "metadata": {},
   "source": [
    "Now, we'll train the posterior for the ground-truth model by training directly on the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e531b0-2329-4c10-b025-d9821af8ca41",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# prepare a posterior distribution\n",
    "features = torch.randn([n_obs, n_latents])\n",
    "\n",
    "linear_map = nn.Linear(n_obs, n_latents)\n",
    "\n",
    "posterior = TrainableDistributionAdapter(MultivariateNormal,\n",
    "                                              linear_map,\n",
    "                                              Covariance(n_latents, rank=5, eps=1e-6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24a758e-6a95-4972-afa5-9501a301e059",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# only training the posterior\n",
    "optim = Adam(posterior.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f093b35a-640d-4199-bbdc-b94d850a73d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    z_sample, x_sample = gt_joint.sample((100,))\n",
    "    logp = posterior.log_prob(z_sample, cond=x_sample).mean()\n",
    "    if i % 100 == 0:\n",
    "        print(f'LogP: {logp:.3f}')\n",
    "    (-logp).backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105cf468-c7a1-4520-b0b8-6b88073a1f67",
   "metadata": {},
   "source": [
    "### Evaluate the posterior\n",
    "Here we will evaluate how good the posterior is (roughly) by sampling $\\hat{z}$ from the posterior $p(z|x)$ and evaluate the expected $\\log p(\\hat{z}, x)$.\n",
    "If $\\hat{z}$ approximates the true distribution over $z$, then $\\log p(\\hat{z}, x)$ will closely apprximate the expected $\\log p(z, x)$, which is negative of entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab93336-46a3-4eff-8ae3-8e2e5bf8f517",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "z_sample, x_sample = gt_joint.sample((1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017c8ad0-0633-4ee9-aba0-431bbd096073",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ground-truth negative entropy\n",
    "gt_joint.log_prob(z_sample, x_sample).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c51f065-1653-454c-b27f-b88b109b86eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# sample from the trained posterior\n",
    "z_hat = posterior.sample(cond=x_sample)\n",
    "\n",
    "# \n",
    "# negative entropy of the approximation\n",
    "gt_joint.log_prob(z_hat, x_sample).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb70952-8651-4a05-8e0a-2bf0f34f434d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
