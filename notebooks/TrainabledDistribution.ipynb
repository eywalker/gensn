{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a8a632de-d9e7-4ea5-ba54-6542235f5a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Normal, MultivariateNormal\n",
    "from torch.optim import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8359f593-177c-443d-9411-23abc8276b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: overall, the handling of returned tuples for the samples is very loose\n",
    "# This stems primarily from the rather inconsitent interface/handling of random variables\n",
    "# of the distribution (can be a singleton/non-tuple OR tuple) vs random variables\n",
    "# you are conditioned on (generally expected to be a tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "id": "99c59a8a-9574-4bea-be63-b6fce61f8068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_to_module(module, field, value):\n",
    "    if isinstance(value, torch.Tensor) and not isinstance(value, nn.Parameter):\n",
    "        # register as buffer\n",
    "        module.register_buffer(field, value)\n",
    "    else:\n",
    "        setattr(module, field, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "id": "e16708a8-4193-4849-8ee3-b8bc841d7535",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainableDistributionAdapter(nn.Module):\n",
    "    \n",
    "    n_rvs = 1\n",
    "    \n",
    "    def __init__(self, distribution_class, *dist_args, _parameters=None, **dist_kwargs):\n",
    "        super().__init__()\n",
    "        self.distrbrituion_class = distribution_class\n",
    "        self.param_counts = len(dist_args)\n",
    "        self.param_keys = list(dist_kwargs.keys())\n",
    "        \n",
    "        for pos, val in enumerate(dist_args):\n",
    "            #setattr(self, f'_arg{pos}', val)\n",
    "            register_to_module(self, f'_arg{pos}', val)\n",
    "        for key, val in dist_kwargs.items():\n",
    "            register_to_module(self, key, val)\n",
    "            \n",
    "        # The alternative interface to allow for a single module to \n",
    "        # flexibly output multiple parameters for the distribution\n",
    "        # at the moment, the module is expected to output a dictionary\n",
    "        # of parameters\n",
    "        if _parameters is not None:\n",
    "            self.parameter_generator = _parameters\n",
    "            \n",
    "    \n",
    "    def distribution(self, cond=None):\n",
    "        cond = turn_to_tuple(cond)\n",
    "        \n",
    "        # a helper function to visit the target field\n",
    "        # and invoke the field with cond if it is a nn.Module.\n",
    "        # Otherwise, simply return the field content\n",
    "        def parse_attr(field, cond=None):\n",
    "            attr = getattr(self, field)\n",
    "            if isinstance(attr, nn.Module):\n",
    "                attr = attr(*cond)\n",
    "            return attr\n",
    "        \n",
    "        dist_args = tuple(parse_attr(f'_arg{pos}', cond=cond) for pos in range(self.param_counts))\n",
    "        dist_kwargs = {k: parse_attr(k, cond=cond) for k in self.param_keys}\n",
    "\n",
    "        # TODO: consider flipping the order of this with\n",
    "        # init specified parameters\n",
    "        if hasattr(self, 'parameter_generator'):\n",
    "            dist_args, dist_kwargs = make_args(self.parameter_generator(*cond), *dist_args, **dist_kwargs)\n",
    "            \n",
    "        return self.distrbrituion_class(*dist_args, **dist_kwargs)\n",
    "    \n",
    "    def log_prob(self, *obs, cond=None):\n",
    "        return self.distribution(cond=cond).log_prob(*obs)\n",
    "    \n",
    "    def forward(self, *obs, cond=None):\n",
    "        return self.log_prob(*obs, cond=cond)\n",
    "    \n",
    "    def sample(self, sample_shape=torch.Size([]), cond=None):\n",
    "        return self.distribution(cond=cond).sample(sample_shape=sample_shape)\n",
    "    \n",
    "    def rsample(self, sample_shape=torch.Size([]), cond=None):\n",
    "        return self.distribution(cond=cond).rsample(sample_shape=sample_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c2a4e9-9498-4a3d-9f70-721ec910b811",
   "metadata": {},
   "source": [
    "y = f(x)\n",
    "\n",
    "p(y) =  det dx/dy p(x)\n",
    "\n",
    "p(y) = 1 / f'(x) p(x)\n",
    "\n",
    "p(f(x)) * f'(x) = p(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "id": "66575a89-89b3-42fe-b1c6-4ad4efe1e867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# come up with a better name\n",
    "# here we assume that conditioning of the joint distribution can occur\n",
    "# by conditioning the prior\n",
    "class Joint(nn.Module):\n",
    "    def __init__(self, prior, conditional):\n",
    "        super().__init__()\n",
    "        self.prior = prior\n",
    "        self.conditional = conditional\n",
    "        self.split = self.prior.n_rvs\n",
    "        self.n_rvs = self.prior.n_rvs + self.conditional.n_rvs\n",
    "        \n",
    "    def log_prob(self, *obs, cond=None):\n",
    "        # TODO: maybe just use self.prior.n_rvs\n",
    "        x, y = obs[:self.split], obs[self.split:]\n",
    "        return self.prior(*x, cond=cond) + self.conditional(*y, cond=x)\n",
    "    \n",
    "    def forward(self, *obs, cond=None):\n",
    "        return self.log_prob(*obs, cond=cond)\n",
    "    \n",
    "    def sample(self, sample_shape=torch.Size([]), cond=None):\n",
    "        x_samples = self.prior.sample(sample_shape=sample_shape, cond=cond)\n",
    "        y_samples = self.conditional.sample(cond=x_samples)\n",
    "        return turn_to_tuple(x_samples) + turn_to_tuple(y_samples)\n",
    "    \n",
    "    def rsample(self, sample_shape=torch.Size([]), cond=None):\n",
    "        x_samples = self.prior.rsample(sample_shape=sample_shape, cond=cond)\n",
    "        y_samples = self.conditional.rsample(cond=x_samples)\n",
    "        return turn_to_tuple(x_samples) + turn_to_tuple(y_samples)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "id": "a6bba839-d7ce-4670-9398-47acfbf414ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conceptual template\n",
    "class InvertibleTransform(nn.Module):\n",
    "    def forward(self, x, logL=0):\n",
    "        return y, logL + log_det_f_prime\n",
    "    \n",
    "    def reverse(self, y, logL=0):\n",
    "        return x, logL - log_det_f_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "id": "99fe6f7e-b4ca-4e96-aa23-a8ebb972d650",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialTransform(nn.Module):\n",
    "    def __init__(self, *transforms):\n",
    "        self.transforms = nn.ModuleList(transforms)\n",
    "        \n",
    "    # TODO: check the content of nn.Module __call__ and create something similar for reverse\n",
    "    def forward(self, x, logL=0, cond=None):\n",
    "        for t in self.transforms:\n",
    "            x, logL = t(x, logL, cond=cond)\n",
    "        return x, logL\n",
    "    \n",
    "    def reverse(self, y, logL=0, cond=None):\n",
    "        for t in self.transforms[::-1]:\n",
    "            y, logL = t(y, logL, cond=cond)\n",
    "        return y, logL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "id": "382e1805-5130-4e16-8508-8e19a216452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowDistribution(nn.Module):\n",
    "    def __init__(self, base_distribution, transform):\n",
    "        super().__init__()\n",
    "        self.base_distribution = base_distribution\n",
    "        self.transform = transform\n",
    "        \n",
    "    def forward(self, *obs, cond=None):\n",
    "        return self.log_prob(*obs, cond=cond)\n",
    "        \n",
    "    def log_prob(self, *obs, cond=None):\n",
    "        x, logL = self.transform.reverse(*obs, cond=cond)\n",
    "        return self.base_distribution.log_prob(*turn_to_tuple(x), cond=cond) + logL\n",
    "    \n",
    "    def sample(self, sample_shape=torch.Size([]), cond=None):\n",
    "        samples = self.base_distribution.sample(sample_shape=sample_shape, cond=cond)\n",
    "        y, _ = self.transform(samples, cond=cond)\n",
    "        return y\n",
    "    \n",
    "    def rsample(self, sample_shape=torch.Size([]), cond=None):\n",
    "        samples = self.base_distribution.rsample(sample_shape=sample_shape, cond=cond)\n",
    "        y, _ = self.transform(samples, cond=cond)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "id": "fe51d144-49cc-418c-b290-4094dd241775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ELBO_joint(joint, posterior, *obs, n_samples=1):\n",
    "    # Joint = p(z, x), Posterior = p(z|x)\n",
    "    z_samples = posterior.rsample((n_samples,), cond=obs)\n",
    "    # take care of case where KL is known for the posterior\n",
    "    elbo = -posterior(*turn_to_tuple(z_samples), cond=obs)\n",
    "    elbo += joint(*turn_to_tuple(z_samples), *obs)\n",
    "    return elbo\n",
    "\n",
    "def ELBO_parts(prior, conditional, posterior, *obs, n_samples=1):\n",
    "    # create a joint\n",
    "    joint = Joint(prior, conditional)\n",
    "    return ELBO_joint(joint, posterior, *obs, n_samples=n_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "id": "7ad619d0-df83-4e4a-bd9e-14751e7cba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELBOMarginal(nn.Module):\n",
    "    def __init__(self, joint, posterior, n_samples=1):\n",
    "        super().__init__()\n",
    "        self.joint = joint\n",
    "        self.posterior = posterior\n",
    "        # infer how many variables are in observations\n",
    "        self.n_rvs = joint.n_rvs - posterior.n_rvs\n",
    "        self.n_samples = n_samples\n",
    "        \n",
    "    def forward(self, *obs, cond=None):\n",
    "        return self.elbo(*obs, cond=cond)\n",
    "        \n",
    "    def elbo(self, *obs, cond=None):\n",
    "        # TODO: deal with conditioning correctly\n",
    "        return ELBO_joint(self.joint, self.posterior, *obs, n_samples=self.n_samples)\n",
    "    \n",
    "    def log_prob(self, *obs):\n",
    "        # TODO: let this be implemented as an \"approximation\" with ELBO\n",
    "        # but with ample warnings\n",
    "        pass\n",
    "    \n",
    "    def sample(self, sample_shape=torch.Size([]), cond=None):\n",
    "        samples = self.joint.sample(sample_shape=sample_shape, cond=cond)\n",
    "        return samples[-self.n_rvs:]\n",
    "    \n",
    "    def rsample(self, sample_shape=torch.Size([]), cond=None):\n",
    "        samples = self.joint.rsample(sample_shape=sample_shape, cond=cond)\n",
    "        return samples[-self.n_rvs:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "id": "5c7499c6-6070-474f-a70a-cfe6e12db41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Consider implementing SurVAE\n",
    "class SurVAE(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "76b4867e-68e5-4657-b45b-950d977c615e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_to_tuple(x):\n",
    "    \"\"\"\n",
    "    Given a value x, turn into a consistent tuple\n",
    "    * if x is None, return an empty tumple ()\n",
    "    * if x is a non-tuple value, return as a single-element tuple (x,)\n",
    "    * if x is already a tuple\n",
    "    \"\"\"\n",
    "    if x is None:\n",
    "        return ()\n",
    "    return x if isinstance(x, tuple) else (x,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "7ace806c-46af-4b96-82f4-7a53206c69b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "bf549bf2-5b98-46a8-900e-96b17e8dc41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_args(x, *args, **kwargs):\n",
    "    if isinstance(x, dict): # TODO: consider making it a Collection.Mapping\n",
    "        kwargs.update(x)\n",
    "    elif isinstance(x, tuple):\n",
    "        args = x + args\n",
    "    else:\n",
    "        args = (x,) + args\n",
    "        \n",
    "    return args, kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "id": "2026682a-166f-4dda-9ecb-686aa54d78e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformedParameter(nn.Module):\n",
    "    def __init__(self, tensor, transform_fn=None):\n",
    "        super().__init__()\n",
    "        self.parameter = nn.Parameter(tensor)\n",
    "        if transform_fn is None:\n",
    "            transform_fn = lambda x: x\n",
    "        self.transform_fn = transform_fn\n",
    "        \n",
    "    @property\n",
    "    def value(self):\n",
    "        return self()\n",
    "        \n",
    "    def forward(self, *args):\n",
    "        return self.transform_fn(self.parameter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "id": "4c4c241b-bafd-4663-b96e-3a8d301fd351",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Covariance(nn.Module):\n",
    "    def __init__(self, n_dims, rank=None, eps=1e-16):\n",
    "        super().__init__()\n",
    "        if rank is None:\n",
    "            rank = n_dims\n",
    "        self.n_dims = n_dims\n",
    "        self.rank = rank\n",
    "        self.eps = eps\n",
    "        self.A = nn.Parameter(torch.randn(n_dims, rank))\n",
    "        \n",
    "    def forward(self, *args):\n",
    "        return self.A @ self.A.T + torch.eye(self.n_dims) * self.eps\n",
    "    \n",
    "    @property\n",
    "    def value(self):\n",
    "        return self()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "90b4dbd2-daf6-4133-92a5-fe873e9617ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: generalize this so that positiveness can arise from other functions\n",
    "class PositiveDiagonal(nn.Module):\n",
    "    def __init__(self, n_dims, eps=1e-16):\n",
    "        super().__init__()\n",
    "        self.n_dims = n_dims\n",
    "        self.eps = eps\n",
    "        self.D = nn.Parameter(torch.randn(n_dims))\n",
    "        \n",
    "    def forward(self, *args):\n",
    "        return torch.diag(self.D**2 + self.eps)\n",
    "    \n",
    "    @property\n",
    "    def value(self):\n",
    "        return self()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "9559a6a1-d356-48ae-b0df-5cd133614709",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbabilisticSIModel(nn.Module):\n",
    "    # TODO: for this to work, the TrainableDistributionAdapter handling of\n",
    "    # _parameters must be expanded. Namely, it needs to be able to accept:\n",
    "    # * positional arguments (to be implemented)\n",
    "    # * dict (already implemented)\n",
    "    # * dict with positional arguments (to be implemented)\n",
    "    # \n",
    "    # To also make this generically useful, it would be helpful to\n",
    "    # allow for output conversion function to be supplied. This function then\n",
    "    # should transform outputs of the SI model into format appropriate\n",
    "    # to serve as _parameters for the TrainableDistributionAdapter\n",
    "    # It's important that such transformation does NOT warp the output\n",
    "    # Doing so will distort the probability density!\n",
    "    def __init__(self, si_model, distribution_class, *dist_args, **dist_kwargs):\n",
    "        super().__init__()\n",
    "        sielf.si_model = si_model\n",
    "        self.distribution_class = distribution_class\n",
    "        self.trainable_distribution = TrainableDistributionAdapter(distribution_class, *dist_args,\n",
    "                                                                   _parameters=si_model, **dist_kwargs)\n",
    "        \n",
    "    def log_prob(self, *obs, cond=None):\n",
    "        return self.trainable_distribution.log_prob(*obs, cond=cond)\n",
    "    \n",
    "    def forward(self, *obs, cond=None):\n",
    "        return self.log_prob(*obs, cond=cond)\n",
    "    \n",
    "    def sample(self, sample_shape=torch.Size([]), cond=None):\n",
    "        return self.trainable_distribution.sample(sample_shape=sample_shape, cond=cond)\n",
    "\n",
    "    def rsample(self, sample_shape=torch.Size([]), cond=None):\n",
    "        return self.trainable_distribution.rsample(sample_shape=sample_shape, cond=cond)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da3a3f0-6496-44c2-877a-51e43049d564",
   "metadata": {},
   "source": [
    "## Example usage: learn Normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82fc8a0-3d0a-4787-8f7e-04bc9b7e4ad5",
   "metadata": {},
   "source": [
    "### Learn only the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "id": "342a96b1-29ef-43ea-bc49-9eb2ad4c2792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the mean & std both learnable\n",
    "# std (scale) is set to be square of a parameter, thus ensuring positive value\n",
    "normal = TrainableDistributionAdapter(Normal, \n",
    "                               loc=nn.Parameter(torch.Tensor([0.0])), \n",
    "                               scale=torch.Tensor([2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "id": "4f047e26-9865-439e-960c-35d10cf87447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('loc', tensor([4.7673])), ('scale', tensor([2.]))])"
      ]
     },
     "execution_count": 680,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "id": "ebdde714-cb26-4019-9a1e-3ed68ad8528b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([4.7673], requires_grad=True)]"
      ]
     },
     "execution_count": 681,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(normal.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "id": "d75b75fc-f810-4ff1-b345-f41665612649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup target normal distribution to learn\n",
    "target = Normal(torch.Tensor([5.0]), torch.Tensor([2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "id": "bbb2a510-507b-4bb9-8f9b-11a3d1b27b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam(normal.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "id": "429c9425-d8bc-488d-b002-ac106a04aac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg logP: 2.102768659591675, mean=tensor([5.0545])\n",
      "Neg logP: 2.129380226135254, mean=tensor([5.4058])\n",
      "Neg logP: 2.053159475326538, mean=tensor([4.9696])\n",
      "Neg logP: 2.0966243743896484, mean=tensor([5.2391])\n",
      "Neg logP: 2.118515968322754, mean=tensor([5.1150])\n",
      "Neg logP: 2.1634061336517334, mean=tensor([4.9011])\n",
      "Neg logP: 2.0691187381744385, mean=tensor([5.4793])\n",
      "Neg logP: 2.030548572540283, mean=tensor([5.2033])\n",
      "Neg logP: 2.1638779640197754, mean=tensor([4.7802])\n",
      "Neg logP: 2.0723798274993896, mean=tensor([5.0352])\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    targets = target.sample((100,))\n",
    "    nlogp = -normal(targets).mean()\n",
    "    mean = normal.loc.detach()\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f'Neg logP: {nlogp}, mean={mean}')\n",
    "    nlogp.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439fe712-7fa3-4a7a-b65c-b21e0332e2b9",
   "metadata": {},
   "source": [
    "### Learn both mean and stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "id": "7809263a-c323-4fe2-bcf1-d9d0ef714c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the mean & std both learnable\n",
    "# std (scale) is set to be square of a parameter, thus ensuring positive value\n",
    "normal = TrainableDistributionAdapter(Normal, \n",
    "                               loc=nn.Parameter(torch.Tensor([0.0])), \n",
    "                               scale=TransformedParameter(torch.Tensor([1.0]), lambda x: x**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "id": "079c5d35-b3b6-41c6-b880-20f05a75f253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup target normal distribution to learn\n",
    "target = Normal(torch.Tensor([5]), torch.Tensor([8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "id": "fe37983e-40d5-4932-8530-7965a4938788",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam(normal.parameters(), lr=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "id": "cbd2836c-e6f0-416f-a851-3bdbd168b0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg logP: 4.520414352416992, mean=tensor([6.4031]), std=tensor([35.6068])\n",
      "Neg logP: 3.544624090194702, mean=tensor([5.6331]), std=tensor([11.0698])\n",
      "Neg logP: 3.5978991985321045, mean=tensor([5.1668]), std=tensor([7.9170])\n",
      "Neg logP: 3.4680280685424805, mean=tensor([5.0932]), std=tensor([7.9356])\n",
      "Neg logP: 3.6048929691314697, mean=tensor([4.9296]), std=tensor([8.2085])\n",
      "Neg logP: 3.5616402626037598, mean=tensor([5.2902]), std=tensor([8.0100])\n",
      "Neg logP: 3.5436019897460938, mean=tensor([5.1700]), std=tensor([8.1878])\n",
      "Neg logP: 3.4725141525268555, mean=tensor([5.0611]), std=tensor([8.1045])\n",
      "Neg logP: 3.451047897338867, mean=tensor([5.1721]), std=tensor([8.2135])\n",
      "Neg logP: 3.495211601257324, mean=tensor([5.1571]), std=tensor([7.8703])\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    targets = target.sample((100,))\n",
    "    nlogp = -normal(targets).mean()\n",
    "    mean = normal.loc.detach()\n",
    "    std = normal.scale.value.detach()\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f'Neg logP: {nlogp}, mean={mean}, std={std}')\n",
    "    nlogp.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ebdfb9-b71b-4131-bff4-afe0cc963aba",
   "metadata": {},
   "source": [
    "### Learn both mean and stdev (same as above, but specified positionally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "id": "e2fd10a4-5e15-479a-8cdf-d79e48014746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the mean & std both learnable\n",
    "# std (scale) is set to be square of a parameter, thus ensuring positive value\n",
    "normal = TrainableDistributionAdapter(Normal, \n",
    "                               nn.Parameter(torch.Tensor([0.0])), \n",
    "                               TransformedParameter(torch.Tensor([1.0]), lambda x: x**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "id": "f3bcbc8b-ae5a-409e-8189-3fc127626849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup target normal distribution to learn\n",
    "target = Normal(torch.Tensor([5]), torch.Tensor([8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "id": "f5a92ccf-bc7a-46d8-ab42-83118e6803be",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam(normal.parameters(), lr=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "id": "0725d790-8b17-4e59-b18b-bd500827adfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg logP: 4.446357250213623, mean=tensor([6.3663]), std=tensor([33.3451])\n",
      "Neg logP: 3.469963312149048, mean=tensor([5.4749]), std=tensor([8.1169])\n",
      "Neg logP: 3.5838122367858887, mean=tensor([4.9612]), std=tensor([8.1047])\n",
      "Neg logP: 3.582165002822876, mean=tensor([5.0933]), std=tensor([8.0675])\n",
      "Neg logP: 3.4827752113342285, mean=tensor([4.7392]), std=tensor([7.8857])\n",
      "Neg logP: 3.4364471435546875, mean=tensor([4.9407]), std=tensor([7.9483])\n",
      "Neg logP: 3.536595106124878, mean=tensor([4.9697]), std=tensor([7.9989])\n",
      "Neg logP: 3.4074501991271973, mean=tensor([5.1103]), std=tensor([7.8625])\n",
      "Neg logP: 3.4930973052978516, mean=tensor([4.9504]), std=tensor([7.6086])\n",
      "Neg logP: 3.5675346851348877, mean=tensor([5.1123]), std=tensor([8.0731])\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    targets = target.sample((100,))\n",
    "    nlogp = -normal(targets).mean()\n",
    "    mean = normal._arg0.detach()\n",
    "    std = normal._arg1.value.detach()\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f'Neg logP: {nlogp}, mean={mean}, std={std}')\n",
    "    nlogp.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057437e7-add2-4d09-99ff-3711dd667f3f",
   "metadata": {},
   "source": [
    "# Conditional case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf10fc1f-e412-410e-abcb-435a4dfd4342",
   "metadata": {},
   "source": [
    "## Simple conditioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c02a881-565f-4acf-a07b-010e0031bc89",
   "metadata": {},
   "source": [
    "Now let us learn more complex relationship $p(z|x)$. Specifically, let $p(z|x) = \\mathcal{N}(f(x), \\sigma^2)$.\n",
    "\n",
    "For simplicity, we'll assume a simple linear mapping for $f(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a03c77-6e88-447a-b848-694c05115cfd",
   "metadata": {},
   "source": [
    "Prepare data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "7d527ba6-0b3f-45da-8d53-d66961bcd9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size):\n",
    "    x = torch.rand((batch_size, 1))\n",
    "    mu = -5 * x + 9\n",
    "    y = Normal(mu, scale=7).sample((batch_size,))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60912ec7-5621-4009-a984-a6da7c464633",
   "metadata": {},
   "source": [
    "Set up the conditional network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "id": "e054cdc5-a040-46eb-893a-c195ed186b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the mean & std both learnable\n",
    "# std (scale) is set to be abs of a parameter, thus ensuring positive value\n",
    "normal = TrainableDistributionAdapter(Normal, \n",
    "                               loc=nn.Linear(1, 1), \n",
    "                               scale=TransformedParameter(torch.Tensor([1.0]), torch.abs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "id": "e73da766-cdc7-4489-8f12-94f5889a71dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam(normal.parameters(), lr=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "aa6c9324-239c-4b22-896c-3438d041e7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg logP: 3.4149293899536133, params={'loc.weight': tensor([[1.2547]]), 'loc.bias': tensor([5.4446]), 'scale.parameter': tensor([7.9149])}\n",
      "Neg logP: 3.379789352416992, params={'loc.weight': tensor([[-0.7773]]), 'loc.bias': tensor([6.6418]), 'scale.parameter': tensor([7.6350])}\n",
      "Neg logP: 3.3700051307678223, params={'loc.weight': tensor([[-2.6216]]), 'loc.bias': tensor([7.6955]), 'scale.parameter': tensor([7.3514])}\n",
      "Neg logP: 3.3667819499969482, params={'loc.weight': tensor([[-3.8872]]), 'loc.bias': tensor([8.3710]), 'scale.parameter': tensor([7.1556])}\n",
      "Neg logP: 3.3841280937194824, params={'loc.weight': tensor([[-4.6002]]), 'loc.bias': tensor([8.7309]), 'scale.parameter': tensor([7.0508])}\n",
      "Neg logP: 3.3676373958587646, params={'loc.weight': tensor([[-4.9019]]), 'loc.bias': tensor([8.9178]), 'scale.parameter': tensor([7.0084])}\n",
      "Neg logP: 3.3664793968200684, params={'loc.weight': tensor([[-4.9266]]), 'loc.bias': tensor([8.9745]), 'scale.parameter': tensor([7.0037])}\n",
      "Neg logP: 3.3688745498657227, params={'loc.weight': tensor([[-4.9868]]), 'loc.bias': tensor([8.9815]), 'scale.parameter': tensor([6.9998])}\n",
      "Neg logP: 3.3621444702148438, params={'loc.weight': tensor([[-4.9526]]), 'loc.bias': tensor([8.9949]), 'scale.parameter': tensor([7.0024])}\n",
      "Neg logP: 3.369663715362549, params={'loc.weight': tensor([[-5.0185]]), 'loc.bias': tensor([8.9876]), 'scale.parameter': tensor([7.0146])}\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    x, y = get_batch(100)\n",
    "    nlogp = -normal(y, cond=x).mean()\n",
    "    params = {k:v.detach() for k,v in normal.state_dict().items()}\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f'Neg logP: {nlogp}, params={params}')\n",
    "    nlogp.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b214cea-c48a-48eb-b416-a70c8450c351",
   "metadata": {},
   "source": [
    "## One network with multiple outputs (returning dict):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "id": "a4583b2d-4a12-475d-9c1c-0dc5dd6be5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalParams(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.core = nn.Linear(1, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        vals = self.core(x)\n",
    "        #loc, scale = vals.split((1,1), dim=1)\n",
    "        return dict(loc=vals[:,0:1], scale=(vals[:, 1:])**2)\n",
    "        #return dict(loc=loc, scale=scale**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "id": "02028ecf-19e7-47bb-8ac5-70bec962af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size):\n",
    "    x = torch.rand((batch_size, 1))\n",
    "    mu = -5 * x + 9\n",
    "    scale = (3 * x + 1)\n",
    "    model = Normal(mu, scale=scale)\n",
    "    y = model.sample((batch_size,))\n",
    "    return x, y, model.log_prob(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "id": "445ebdae-679a-4ae7-ba4a-250732bd645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the mean & std both learnable\n",
    "# std (scale) is set to be square of a parameter, thus ensuring positive value\n",
    "normal = TrainableDistributionAdapter(Normal, _parameters=NormalParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "id": "68a3c3e8-d032-4c6a-98fc-b5aaf6e1ca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam(normal.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "id": "ab641be3-668e-434e-8c7b-c39bbee50871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg logP: 2.214 (gt=2.213), params={'parameter_generator.core.weight': tensor([[-5.0274],\n",
      "        [-0.9990]]), 'parameter_generator.core.bias': tensor([ 9.0434, -1.0585])}\n",
      "Neg logP: 2.227 (gt=2.225), params={'parameter_generator.core.weight': tensor([[-4.9925],\n",
      "        [-0.9860]]), 'parameter_generator.core.bias': tensor([ 8.9737, -1.0683])}\n",
      "Neg logP: 2.324 (gt=2.324), params={'parameter_generator.core.weight': tensor([[-5.0019],\n",
      "        [-1.0087]]), 'parameter_generator.core.bias': tensor([ 9.0111, -1.0616])}\n",
      "Neg logP: 2.262 (gt=2.260), params={'parameter_generator.core.weight': tensor([[-4.9992],\n",
      "        [-0.9894]]), 'parameter_generator.core.bias': tensor([ 9.0099, -1.0683])}\n",
      "Neg logP: 2.291 (gt=2.290), params={'parameter_generator.core.weight': tensor([[-5.0182],\n",
      "        [-1.0027]]), 'parameter_generator.core.bias': tensor([ 8.9757, -1.0533])}\n",
      "Neg logP: 2.226 (gt=2.224), params={'parameter_generator.core.weight': tensor([[-4.9768],\n",
      "        [-1.0186]]), 'parameter_generator.core.bias': tensor([ 8.9651, -1.0622])}\n",
      "Neg logP: 2.364 (gt=2.362), params={'parameter_generator.core.weight': tensor([[-5.0832],\n",
      "        [-1.0339]]), 'parameter_generator.core.bias': tensor([ 9.0106, -1.0542])}\n",
      "Neg logP: 2.319 (gt=2.317), params={'parameter_generator.core.weight': tensor([[-5.0150],\n",
      "        [-0.9906]]), 'parameter_generator.core.bias': tensor([ 8.9750, -1.0691])}\n",
      "Neg logP: 2.300 (gt=2.297), params={'parameter_generator.core.weight': tensor([[-5.0361],\n",
      "        [-0.9898]]), 'parameter_generator.core.bias': tensor([ 8.9865, -1.0536])}\n",
      "Neg logP: 2.306 (gt=2.304), params={'parameter_generator.core.weight': tensor([[-4.9455],\n",
      "        [-0.9946]]), 'parameter_generator.core.bias': tensor([ 8.9759, -1.0532])}\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    x, y, true_logp = get_batch(100)\n",
    "    nlogp = -normal(y, cond=x).mean()\n",
    "    params = {k:v.detach() for k,v in normal.state_dict().items()}\n",
    "    if i % 100 == 0:\n",
    "        print(f'Neg logP: {nlogp:0.3f} (gt={-true_logp.mean():0.3f}), params={params}')\n",
    "    nlogp.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700ce5da-dd6b-41b4-8369-bb1414bf2b81",
   "metadata": {},
   "source": [
    "## One network with multiple outputs (returning positionally):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "id": "012d4071-af9a-4486-90fc-8d27a6c4f38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalParams(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.core = nn.Linear(1, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        vals = self.core(x)\n",
    "        #loc, scale = vals.split((1,1), dim=1)\n",
    "        return vals[:,0:1], (vals[:, 1:])**2\n",
    "        #return dict(loc=loc, scale=scale**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "a8001caf-c7e7-4f01-a53d-d5d73046e32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size):\n",
    "    x = torch.rand((batch_size, 1))\n",
    "    mu = -5 * x + 9\n",
    "    scale = (3 * x + 1)\n",
    "    model = Normal(mu, scale=scale)\n",
    "    y = model.sample((batch_size,))\n",
    "    return x, y, model.log_prob(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "id": "233f1a5f-086e-46c4-b512-bdf177a3f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the mean & std both learnable\n",
    "# std (scale) is set to be square of a parameter, thus ensuring positive value\n",
    "normal = TrainableDistributionAdapter(Normal, _parameters=NormalParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "id": "0f74d010-d81d-42fc-a405-fe327cfdbd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam(normal.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "id": "d378f8c8-bde5-409c-a1bf-765aa0fb98a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg logP: 44.716 (gt=2.301), params={'parameter_generator.core.weight': tensor([[-0.5920],\n",
      "        [ 0.8770]]), 'parameter_generator.core.bias': tensor([0.3602, 0.6186])}\n",
      "Neg logP: 5.115 (gt=2.217), params={'parameter_generator.core.weight': tensor([[5.5234],\n",
      "        [4.9245]]), 'parameter_generator.core.bias': tensor([6.5077, 5.9420])}\n",
      "Neg logP: 3.884 (gt=2.244), params={'parameter_generator.core.weight': tensor([[5.0220],\n",
      "        [0.1505]]), 'parameter_generator.core.bias': tensor([6.3939, 4.2745])}\n",
      "Neg logP: 2.596 (gt=2.285), params={'parameter_generator.core.weight': tensor([[-1.9150],\n",
      "        [-0.6146]]), 'parameter_generator.core.bias': tensor([6.8716, 2.2467])}\n",
      "Neg logP: 2.265 (gt=2.263), params={'parameter_generator.core.weight': tensor([[-5.0224],\n",
      "        [ 1.0395]]), 'parameter_generator.core.bias': tensor([8.9794, 1.0392])}\n",
      "Neg logP: 2.344 (gt=2.343), params={'parameter_generator.core.weight': tensor([[-5.0002],\n",
      "        [ 1.0129]]), 'parameter_generator.core.bias': tensor([8.9962, 1.0518])}\n",
      "Neg logP: 2.273 (gt=2.272), params={'parameter_generator.core.weight': tensor([[-4.9962],\n",
      "        [ 1.0160]]), 'parameter_generator.core.bias': tensor([8.9894, 1.0458])}\n",
      "Neg logP: 2.268 (gt=2.267), params={'parameter_generator.core.weight': tensor([[-5.0022],\n",
      "        [ 1.0056]]), 'parameter_generator.core.bias': tensor([9.0243, 1.0498])}\n",
      "Neg logP: 2.235 (gt=2.233), params={'parameter_generator.core.weight': tensor([[-4.9992],\n",
      "        [ 1.0099]]), 'parameter_generator.core.bias': tensor([8.9973, 1.0572])}\n",
      "Neg logP: 2.214 (gt=2.213), params={'parameter_generator.core.weight': tensor([[-5.0166],\n",
      "        [ 1.0072]]), 'parameter_generator.core.bias': tensor([9.0130, 1.0551])}\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    x, y, true_logp = get_batch(100)\n",
    "    nlogp = -normal(y, cond=x).mean()\n",
    "    params = {k:v.detach() for k,v in normal.state_dict().items()}\n",
    "    if i % 100 == 0:\n",
    "        print(f'Neg logP: {nlogp:0.3f} (gt={-true_logp.mean():0.3f}), params={params}')\n",
    "    nlogp.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf2da13-725c-4f64-af19-1432284474c0",
   "metadata": {},
   "source": [
    "# Test sampling and joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "id": "11f3cc5a-da5f-4b28-8f33-ab7f44b38f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = TrainableDistributionAdapter(Normal, loc=nn.Parameter(torch.Tensor([5])),\n",
    "                                     scale=torch.Tensor([2]))\n",
    "\n",
    "linear = nn.Linear(1, 1)\n",
    "linear.weight.data = torch.Tensor([[-2]])\n",
    "linear.bias.data = torch.Tensor([6])\n",
    "conditional = TrainableDistributionAdapter(Normal, loc=linear, scale=torch.Tensor([1]))\n",
    "\n",
    "# create a joint distribution out of prior and conditional\n",
    "joint = Joint(prior, conditional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "id": "1b33e734-1f33-4920-b997-864033282b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = joint.sample((10000,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5278584-1f7f-4741-a535-fb4115d17e24",
   "metadata": {},
   "source": [
    "Should be 5, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "id": "7c4ae965-786b-4cd3-8e29-dfbe404c87b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4.9870), tensor(2.0324))"
      ]
     },
     "execution_count": 710,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9ae6a0-b85f-4e96-87f1-8202de075983",
   "metadata": {},
   "source": [
    "Should be -4, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "id": "3440de7f-4bb6-4acb-9155-ca9abaa46e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-3.9677), tensor(4.1786))"
      ]
     },
     "execution_count": 711,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.mean(), y.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fcba96-c6d3-4d27-aaf0-322efa2c69c1",
   "metadata": {},
   "source": [
    "# Simulating a multi-dimensional joint distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a202a9f-482b-44a6-bc01-64674e729ec7",
   "metadata": {},
   "source": [
    "### Define the ground-truth generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "id": "654699e9-7213-4aa2-81a0-cb264b8e1ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_latents = 5\n",
    "gt_prior = TrainableDistributionAdapter(MultivariateNormal, \n",
    "                                        torch.ones([n_latents]),\n",
    "                                        torch.eye(n_latents))\n",
    "\n",
    "n_obs = 10\n",
    "features = torch.randn([n_latents, n_obs])\n",
    "\n",
    "feature_map = nn.Linear(n_latents, n_obs)\n",
    "feature_map.weight.data = features.T\n",
    "feature_map.bias.data.zero_()\n",
    "\n",
    "gt_conditional = TrainableDistributionAdapter(MultivariateNormal,\n",
    "                                              feature_map,\n",
    "                                              torch.eye(n_obs))\n",
    "\n",
    "gt_joint = Joint(gt_prior, gt_conditional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670797ef-bada-4481-8084-660ec61fbc15",
   "metadata": {},
   "source": [
    "### Now prepare a trainable model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fd1342-4ac6-419a-aef6-b9c9e92a2d16",
   "metadata": {},
   "source": [
    "class Covariance(nn.Module):\n",
    "    def __init__(self, n_dim):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "id": "04f84ea4-6f14-499a-a42a-a789a4ddcd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_latents = 5\n",
    "model_prior = TrainableDistributionAdapter(MultivariateNormal, \n",
    "                                        nn.Parameter(torch.zeros([n_latents])),\n",
    "                                        PositiveDiagonal(n_latents))\n",
    "\n",
    "n_obs = 10\n",
    "features = torch.randn([n_latents, n_obs])\n",
    "\n",
    "feature_map = nn.Linear(n_latents, n_obs)\n",
    "#feature_map.weight.data = features.T\n",
    "#feature_map.bias.data.zero_()\n",
    "\n",
    "model_conditional = TrainableDistributionAdapter(MultivariateNormal,\n",
    "                                              feature_map,\n",
    "                                              torch.eye(n_obs))\n",
    "\n",
    "model_joint = Joint(model_prior, model_conditional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5aae5e-7c0f-4c05-be8e-a79c7b919d61",
   "metadata": {},
   "source": [
    "### Go ahead and train the joint model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "id": "6e0aa86e-e17f-4e8d-ad65-9721841e0186",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam(model_joint.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "id": "92a32c09-c099-4593-b365-06f0a7f6000c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logp: -22.338 / GT logp: -20.972\n",
      "Model logp: -22.691 / GT logp: -21.712\n",
      "Model logp: -22.532 / GT logp: -21.455\n",
      "Model logp: -22.455 / GT logp: -21.301\n",
      "Model logp: -22.525 / GT logp: -21.508\n",
      "Model logp: -22.324 / GT logp: -21.521\n",
      "Model logp: -22.413 / GT logp: -21.539\n",
      "Model logp: -21.608 / GT logp: -20.863\n",
      "Model logp: -22.002 / GT logp: -21.358\n",
      "Model logp: -22.456 / GT logp: -21.556\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    samples = gt_joint.sample((100,))\n",
    "    gt_logl = gt_joint.log_prob(*samples).mean()\n",
    "    model_logl = model_joint.log_prob(*samples).mean()\n",
    "    if i % 100 == 0:\n",
    "        print(f'Model logp: {model_logl:.3f} / GT logp: {gt_logl:.3f}')\n",
    "    (-model_logl).backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23276263-918b-43d7-aca9-f18586b71b66",
   "metadata": {},
   "source": [
    "Checking the learned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "id": "2d55a492-57e4-4586-840f-f6619554a769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2909, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.7904, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 1.3943, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.9610, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.7481]], grad_fn=<DiagBackward0>)"
      ]
     },
     "execution_count": 717,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_joint.prior._arg1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "id": "b647e339-4091-4a06-bbf8-07e326e6dd76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('_arg0', tensor([0.8104, 1.0067, 0.9980, 0.9421, 0.9557])),\n",
       "             ('_arg1.D',\n",
       "              tensor([ 1.1362,  0.8890,  1.1808,  0.9803, -0.8649]))])"
      ]
     },
     "execution_count": 718,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_joint.prior.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "id": "d3b85c30-7306-4863-88d3-db6f66b8d720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('_arg1',\n",
       "              tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "                      [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "                      [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "                      [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "                      [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "                      [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "                      [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])),\n",
       "             ('_arg0.weight',\n",
       "              tensor([[ 0.5283, -0.6021,  1.1344, -0.6800, -0.1336],\n",
       "                      [ 1.0862, -0.6126,  0.3790, -1.7255,  0.7884],\n",
       "                      [-0.8417,  0.0308, -0.9640,  1.5167, -1.3290],\n",
       "                      [-0.9635,  1.1864, -0.8965,  0.8684, -1.0919],\n",
       "                      [ 0.7885, -1.1611, -0.1120,  0.8234, -0.6543],\n",
       "                      [-0.0067,  0.9101, -2.4270, -0.8742,  0.9024],\n",
       "                      [-0.9694,  0.5877, -0.3937, -1.9460, -0.5441],\n",
       "                      [ 1.8141, -0.5857,  1.3297,  1.2805,  1.2053],\n",
       "                      [-0.7943, -0.8235,  0.2352,  1.0708, -0.5590],\n",
       "                      [-0.3058,  0.7176, -1.7502, -1.3832, -0.0623]])),\n",
       "             ('_arg0.bias',\n",
       "              tensor([ 0.0723,  0.0467, -0.0691, -0.0744, -0.0825, -0.1052, -0.0338,  0.0814,\n",
       "                       0.0768, -0.1379]))])"
      ]
     },
     "execution_count": 719,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_joint.conditional.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1e676c-2beb-48f4-ba7a-16e6ddc4dfe6",
   "metadata": {},
   "source": [
    "## Compute the posterior distriubtion via ELBO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1721c353-3fc8-48a6-9dd2-3076db090b9b",
   "metadata": {},
   "source": [
    "First, we'll train the posterior for the ground-truth model, using ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "id": "e8a1d168-d44e-4981-9377-44d8eadb6aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a posterior distribution\n",
    "features = torch.randn([n_obs, n_latents])\n",
    "\n",
    "linear_map = nn.Linear(n_obs, n_latents)\n",
    "\n",
    "posterior = TrainableDistributionAdapter(MultivariateNormal,\n",
    "                                              linear_map,\n",
    "                                              Covariance(n_latents, rank=5, eps=1e-6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "id": "f692ccc5-cbbe-4c2c-bea3-ba29d6f0e9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo_x = ELBOMarginal(gt_joint, posterior)\n",
    "\n",
    "# only training the posterior\n",
    "optim = Adam(posterior.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "id": "9b3ead08-3a46-495b-af06-7b56d838b713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model elbo: -267.320\n",
      "Model elbo: -42.153\n",
      "Model elbo: -22.208\n",
      "Model elbo: -19.550\n",
      "Model elbo: -19.352\n",
      "Model elbo: -19.348\n",
      "Model elbo: -19.341\n",
      "Model elbo: -19.347\n",
      "Model elbo: -19.351\n",
      "Model elbo: -19.351\n"
     ]
    }
   ],
   "source": [
    "z_sample, x_sample = gt_joint.sample((1000,))\n",
    "\n",
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    elbo = elbo_x(x_sample).mean()\n",
    "    if i % 100 == 0:\n",
    "        print(f'Model elbo: {elbo:.3f}')\n",
    "    (-elbo).backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76d032b-2ed2-4e09-a803-69f0463a4e0b",
   "metadata": {},
   "source": [
    "### Evaluate the posterior\n",
    "Here we will evaluate how good the posterior is (roughly) by sampling $\\hat{z}$ from the posterior $p(z|x)$ and evaluate the expected $\\log p(\\hat{z}, x)$.\n",
    "If $\\hat{z}$ approximates the true distribution over $z$, then $\\log p(\\hat{z}, x)$ will closely apprximate the expected $\\log p(z, x)$, which is negative of entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "id": "8581b914-7f75-4381-b3ec-d9acb6d486d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_sample, x_sample = gt_joint.sample((1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "id": "07fea8fa-e733-4c3c-8a68-efeb80c95dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-21.1714, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 724,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ground-truth negative entropy\n",
    "gt_joint.log_prob(z_sample, x_sample).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "id": "5f49637e-67c9-4107-a824-7f56e93ac040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-21.2454, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 725,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample from the trained posterior\n",
    "z_hat = posterior.sample(cond=x_sample)\n",
    "\n",
    "# \n",
    "# negative entropy of the approximation\n",
    "gt_joint.log_prob(z_hat, x_sample).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b38dcd-8b0d-421f-a5ba-d70ff18656b0",
   "metadata": {},
   "source": [
    "## Compute the posterior distriubtion via direct fit to samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1684cb3d-c162-4308-8c09-81cb7caf746f",
   "metadata": {},
   "source": [
    "Now, we'll train the posterior for the ground-truth model by training directly on the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "id": "21e531b0-2329-4c10-b025-d9821af8ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a posterior distribution\n",
    "features = torch.randn([n_obs, n_latents])\n",
    "\n",
    "linear_map = nn.Linear(n_obs, n_latents)\n",
    "\n",
    "posterior = TrainableDistributionAdapter(MultivariateNormal,\n",
    "                                              linear_map,\n",
    "                                              Covariance(n_latents, rank=5, eps=1e-6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "id": "d24a758e-6a95-4972-afa5-9501a301e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only training the posterior\n",
    "optim = Adam(posterior.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "id": "f093b35a-640d-4199-bbdc-b94d850a73d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogP: -34.669\n",
      "LogP: -5.655\n",
      "LogP: -4.524\n",
      "LogP: -3.645\n",
      "LogP: -2.407\n",
      "LogP: -2.074\n",
      "LogP: -2.334\n",
      "LogP: -1.957\n",
      "LogP: -1.962\n",
      "LogP: -2.228\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    z_sample, x_sample = gt_joint.sample((100,))\n",
    "    logp = posterior.log_prob(z_sample, cond=x_sample).mean()\n",
    "    if i % 100 == 0:\n",
    "        print(f'LogP: {logp:.3f}')\n",
    "    (-logp).backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105cf468-c7a1-4520-b0b8-6b88073a1f67",
   "metadata": {},
   "source": [
    "### Evaluate the posterior\n",
    "Here we will evaluate how good the posterior is (roughly) by sampling $\\hat{z}$ from the posterior $p(z|x)$ and evaluate the expected $\\log p(\\hat{z}, x)$.\n",
    "If $\\hat{z}$ approximates the true distribution over $z$, then $\\log p(\\hat{z}, x)$ will closely apprximate the expected $\\log p(z, x)$, which is negative of entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "id": "4ab93336-46a3-4eff-8ae3-8e2e5bf8f517",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_sample, x_sample = gt_joint.sample((1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "id": "017c8ad0-0633-4ee9-aba0-431bbd096073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-21.3203, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 736,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ground-truth negative entropy\n",
    "gt_joint.log_prob(z_sample, x_sample).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "id": "8c51f065-1653-454c-b27f-b88b109b86eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-21.5950, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 737,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample from the trained posterior\n",
    "z_hat = posterior.sample(cond=x_sample)\n",
    "\n",
    "# \n",
    "# negative entropy of the approximation\n",
    "gt_joint.log_prob(z_hat, x_sample).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb70952-8651-4a05-8e0a-2bf0f34f434d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
